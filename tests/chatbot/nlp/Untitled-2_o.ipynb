{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# project_root = Path(__file__).resolve().parents[1]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# sys.path.append(str(project_root))\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mncomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rlst, srlst, clst, glst, rrlst, dtlst, sslst\n\u001b[0;32m     16\u001b[0m paths \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data/storage/processed\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124modata\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data/storage/processed/final_cleaning.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m spec \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_cleaning.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from rapidfuzz import fuzz, process\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.ncomp import rlst, srlst, clst, glst, rrlst, dtlst, sslst\n",
    "\n",
    "paths = {\n",
    "    \"processed\": os.path.abspath(f\"{project_root}/data/storage/processed\"),\n",
    "    \"odata\": os.path.abspath(f\"{project_root}/data/storage/processed/final_cleaning.csv\"),\n",
    "}\n",
    "spec = pd.read_csv(f\"{paths['processed']}/final_cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network nâng cấp với Batch Normalization và Dropout để tinh chỉnh embeddings.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Kích thước đầu vào.\n",
    "        hidden_size (int): Số nơ-ron ở lớp ẩn.\n",
    "        num_classes (int): Kích thước đầu ra (trong trường hợp này bằng input_size).\n",
    "        dropout_rate (float): Tỉ lệ dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "# ---------------------------\n",
    "# ComponentExtractor với Ensemble và NN refinement\n",
    "# ---------------------------\n",
    "class ComponentExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = 0.45,\n",
    "        fuzzy_threshold: int = 60,\n",
    "        use_nn: bool = True,\n",
    "        embedding_dim: int = 384,  # kích thước của embedding từ SentenceTransformer \"all-MiniLM-L6-v2\"\n",
    "        hidden_size: int = 256\n",
    "    ) -> None:\n",
    "        # Load spec từ file (ở đây giả sử spec đã được load từ paths[\"odata\"])\n",
    "        # Chúng ta không sử dụng \"brand\" trong trích xuất vì nó được lấy từ spec.\n",
    "        self.spec = None  # Nếu cần, spec sẽ được sử dụng bởi lớp PostProcessor riêng.\n",
    "        \n",
    "        # Khai báo 2 mô hình SentenceTransformer (ensemble)\n",
    "        self.models = [\n",
    "            SentenceTransformer(\"all-mpnet-base-v2\"),\n",
    "            SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        ]\n",
    "        # Các danh mục cần trích xuất (không bao gồm \"brand\")\n",
    "        self.components = {\n",
    "            \"gpu\": sorted(glst(), key=len, reverse=False),\n",
    "            \"cpu\": sorted(clst(), key=len, reverse=False),\n",
    "            \"ram\": sorted(rlst(), key=len, reverse=False),\n",
    "            \"resolution\": sorted(srlst(), key=len, reverse=True),\n",
    "            \"refresh rate\": sorted(rrlst(), key=len, reverse=False),\n",
    "            \"display type\": sorted(dtlst(), key=len, reverse=False),\n",
    "            \"screen size\": sorted(sslst(), key=len, reverse=False),\n",
    "        }\n",
    "        # Tạo DataFrame từ dictionary (các cột dạng chữ thường)\n",
    "        self.df_components = pd.DataFrame.from_dict(self.components, orient=\"index\").transpose()\n",
    "        self.df_components.columns = [col.lower() for col in self.df_components.columns]\n",
    "        \n",
    "        # Pre-compute embeddings cho từng danh mục cho từng mô hình\n",
    "        self.embeddings = []  # danh sách chứa dictionary embeddings cho mỗi mô hình\n",
    "        for model in self.models:\n",
    "            model_emb = {}\n",
    "            for comp_name, comp_list in self.components.items():\n",
    "                cleaned = [self._clean_text(text) for text in comp_list]\n",
    "                model_emb[comp_name] = model.encode(cleaned, convert_to_tensor=True)\n",
    "            self.embeddings.append(model_emb)\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        self.fuzzy_threshold = fuzzy_threshold\n",
    "        self.use_nn = use_nn\n",
    "        if self.use_nn:\n",
    "            self.embedding_nets = {}\n",
    "            for comp_name in self.components.keys():\n",
    "                # Khởi tạo một NeuralNetwork cho mỗi danh mục\n",
    "                self.embedding_nets[comp_name] = NeuralNetwork(input_size=embedding_dim, hidden_size=hidden_size, num_classes=embedding_dim, dropout_rate=0.3)\n",
    "                # Ở chế độ inference, ta để chúng ở eval; khi fine-tuning có thể đặt train()\n",
    "                self.embedding_nets[comp_name].eval()\n",
    "        \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Chuyển thành chữ thường, thay '-' và '/' bằng khoảng trắng, loại bỏ ký tự không cần thiết.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[\\-/]\", \" \", text)\n",
    "        text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    def _fuzzy_match(self, candidates: List[str], query: str) -> Optional[str]:\n",
    "        \"\"\"Sử dụng fuzzy matching để tìm candidate tốt nhất từ danh sách.\"\"\"\n",
    "        best_candidate = None\n",
    "        best_score = 0\n",
    "        for cand in candidates:\n",
    "            score = fuzz.partial_ratio(query, cand)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_candidate = cand\n",
    "        if best_score >= self.fuzzy_threshold:\n",
    "            return best_candidate\n",
    "        return None\n",
    "\n",
    "    def _ensemble_cosine(self, comp_name: str, query_embedding_list: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Tính trung bình trọng số (ensemble) của cosine similarity từ các mô hình.\"\"\"\n",
    "        # Lấy danh sách điểm cosine từ từng mô hình\n",
    "        scores_list = []\n",
    "        for idx, model_emb in enumerate(self.embeddings):\n",
    "            candidate_emb = model_emb[comp_name]\n",
    "            # Nếu sử dụng NN refinement, chuyển đổi candidate embedding\n",
    "            if self.use_nn:\n",
    "                net = self.embedding_nets[comp_name]\n",
    "                candidate_emb = net(candidate_emb)\n",
    "                query_emb = net(query_embedding_list[idx])\n",
    "            else:\n",
    "                query_emb = query_embedding_list[idx]\n",
    "            # Tính cosine similarity\n",
    "            dot = torch.matmul(candidate_emb, query_emb.unsqueeze(1)).squeeze()\n",
    "            norm_candidate = torch.norm(candidate_emb, dim=1)\n",
    "            norm_query = torch.norm(query_emb)\n",
    "            cos_scores = dot / (norm_candidate * norm_query + 1e-8)\n",
    "            scores_list.append(cos_scores)\n",
    "        # Kết hợp theo trọng số, ví dụ: [0.6, 0.4]\n",
    "        weights = [0.6, 0.4]\n",
    "        combined = sum(w * s for w, s in zip(weights, scores_list))\n",
    "        return combined\n",
    "\n",
    "    def extract_components(self, new_question: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Trích xuất các component từ new_question.\n",
    "        Sử dụng ensemble của các mô hình SentenceTransformer, kết hợp với NN refinement nếu enabled.\n",
    "        Nếu cosine similarity không đạt, fallback qua fuzzy matching.\n",
    "        \"\"\"\n",
    "        cleaned_query = self._clean_text(new_question)\n",
    "        # Tính embedding của câu hỏi từ mỗi mô hình\n",
    "        query_embeddings = [model.encode(cleaned_query, convert_to_tensor=True) for model in self.models]\n",
    "        # Đưa tất cả query embeddings về cùng device với các embeddings (giả sử model đầu tiên)\n",
    "        device = self.embeddings[0][next(iter(self.embeddings[0]))].device\n",
    "        query_embeddings = [emb.to(device) for emb in query_embeddings]\n",
    "        \n",
    "        results = {}\n",
    "        for comp_name, candidates in self.components.items():\n",
    "            if not candidates:\n",
    "                results[comp_name] = None\n",
    "                continue\n",
    "            combined_scores = self._ensemble_cosine(comp_name, query_embeddings)\n",
    "            best_score, best_idx = torch.max(combined_scores, dim=0)\n",
    "            candidate = candidates[best_idx.item()] if best_score.item() >= self.threshold else None\n",
    "            if candidate is None:\n",
    "                candidate = self._fuzzy_match(candidates, new_question.lower())\n",
    "            results[comp_name] = candidate\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessor:\n",
    "    def __init__(self) -> None:\n",
    "        # Đọc file spec và chuyển tên cột về chữ thường\n",
    "        self.spec = pd.read_csv(paths[\"odata\"])\n",
    "        self.spec.columns = [col.lower() for col in self.spec.columns]\n",
    "\n",
    "        for col in [\"gpu\", \"cpu\", \"brand\", \"ram\", \"resolution\", \"refresh rate\", \"display type\", \"screen size\"]:\n",
    "            if col in self.spec.columns:\n",
    "                self.spec[col] = self.spec[col].astype(str).str.lower()\n",
    "\n",
    "    def process_gpu(self, detected_gpu: str, new_question: str) -> list:\n",
    "        \"\"\"\n",
    "        Xử lý GPU:\n",
    "        - Nếu detected_gpu (ví dụ: \"geforce rtx 3070 ti\") xuất hiện nguyên vẹn trong new_question, \n",
    "            thì sử dụng nó để lọc trong spec.\n",
    "        - Nếu không, tách thành các token và sử dụng cửa sổ trượt để tìm chuỗi con dài nhất xuất hiện trong new_question.\n",
    "        - Sau đó, lọc trong spec theo chuỗi con tìm được và trả về danh sách các tên GPU tương ứng.\n",
    "        \"\"\"\n",
    "        new_q = new_question.lower()\n",
    "        if not detected_gpu:\n",
    "            return None\n",
    "        \n",
    "        # Nếu detected_gpu xuất hiện nguyên vẹn trong new_question\n",
    "        if detected_gpu in new_q:\n",
    "            filtered_df = self.spec[self.spec[\"gpu\"].str.contains(detected_gpu, case=False, na=False)]\n",
    "            if not filtered_df.empty:\n",
    "                return filtered_df[\"gpu\"].unique().tolist()\n",
    "            else:\n",
    "                return [detected_gpu]\n",
    "        \n",
    "        # Nếu không, tách detected_gpu thành các token\n",
    "        tokens = detected_gpu.split()\n",
    "        best_candidate = \"\"\n",
    "        \n",
    "        # Duyệt từng vị trí bắt đầu\n",
    "        for i in range(len(tokens)):\n",
    "            candidate = \"\"\n",
    "            # Duyệt cửa sổ từ vị trí i đến hết\n",
    "            for j in range(i, len(tokens)):\n",
    "                candidate = \" \".join(tokens[i:j+1])\n",
    "                if candidate in new_q:\n",
    "                    # Lưu lại candidate nếu dài hơn candidate đã lưu\n",
    "                    if len(candidate) > len(best_candidate):\n",
    "                        best_candidate = candidate\n",
    "                else:\n",
    "                    # Nếu candidate không có, break cửa sổ hiện tại\n",
    "                    break\n",
    "\n",
    "        if best_candidate:\n",
    "            filtered_df = self.spec[self.spec[\"gpu\"].str.contains(best_candidate, case=False, na=False)]\n",
    "            if not filtered_df.empty:\n",
    "                return filtered_df[\"gpu\"].unique().tolist()\n",
    "            else:\n",
    "                return [best_candidate]\n",
    "        return None\n",
    "\n",
    "    def process_cpu(self, detected_cpu: str, new_question: str) -> list:\n",
    "        \"\"\"\n",
    "        Loại bỏ \"th\" và \"gen\", sau đó kiểm tra nếu detected_cpu không có trong new_question,\n",
    "        tách thành các token và duyệt dần để tạo chuỗi con.\n",
    "        Sau đó, lọc trong spec theo cột 'cpu' và chỉ trả về danh sách cuối cùng.\n",
    "        \"\"\"\n",
    "        new_q = new_question.lower()\n",
    "        if not detected_cpu:\n",
    "            return None\n",
    "        normalized = detected_cpu.lower().replace(\"th\", \"\").replace(\"gen\", \"\").strip()\n",
    "        filtered_df = self.spec[self.spec[\"cpu\"].str.contains(normalized, case=False, na=False)]\n",
    "        if normalized in new_q:\n",
    "            filtered_df = self.spec[self.spec[\"cpu\"].str.contains(normalized, case=False, na=False)]\n",
    "            if not filtered_df.empty:\n",
    "                return filtered_df[\"cpu\"].unique().tolist()\n",
    "        tokens = normalized.split()\n",
    "        candidate = \"\"\n",
    "        found_candidate = None\n",
    "        for token in tokens:\n",
    "            candidate = (candidate + \" \" + token).strip()\n",
    "            if candidate in new_q:\n",
    "                found_candidate = candidate\n",
    "            else:\n",
    "                break\n",
    "        if found_candidate:\n",
    "            filtered_df = self.spec[self.spec[\"cpu\"].str.contains(found_candidate, case=False, na=False)]\n",
    "            if not filtered_df.empty:\n",
    "                return filtered_df[\"cpu\"].unique().tolist()\n",
    "        return filtered_df[\"cpu\"].unique().tolist() if not filtered_df.empty else None\n",
    "\n",
    "    def process_resolution(self, detected_res: str) -> str:\n",
    "        \"\"\"\n",
    "        Xử lý resolution:\n",
    "          - Loại bỏ các từ khóa như \"display\", \"resolution\", ...\n",
    "          - Sử dụng kỹ thuật fuzzy matching để xác định key trong dict.\n",
    "        \"\"\"\n",
    "        if not detected_res:\n",
    "            return None\n",
    "        keywords = [\"display\", \"resolution\", \"display panel\", \"display resolution\", \"screen resolution\", \"monitor resolution\"]\n",
    "        comp = detected_res.lower()\n",
    "        for kw in keywords:\n",
    "            comp = comp.replace(kw, \"\")\n",
    "        comp = comp.strip()\n",
    "        resolution_dict = {\n",
    "            \"3072 x 1920\": [\"3072 x 1920\", \"3k\", \"3072p\", \"triple hd\"],\n",
    "            \"1920 x 1200\": [\"1920 x 1200\", \"wuxga\", \"16 10 hd+\", \"hd+ 16 10\"],\n",
    "            \"2560 x 1600\": [\"2560 x 1600\", \"wqxga\", \"quad extended 16 10\", \"retina-like\"],\n",
    "            \"2560 x 1440\": [\"2560 x 1440\", \"qhd\", \"quad hd\", \"2k\", \"wqhd\"],\n",
    "            \"1920 x 1080\": [\"1920 x 1080\", \"fhd\", \"full hd\", \"1080p\"],\n",
    "            \"3840 x 2160\": [\"3840 x 2160\", \"4k uhd\", \"4k\", \"uhd\", \"ultra hd\", \"2160p\"],\n",
    "            \"2880 x 1800\": [\"2880 x 1800\", \"retina 15\", \"qhd+ 16 10\"],\n",
    "            \"3840 x 2400\": [\"3840 x 2400\", \"wquxga\", \"16 10 4k+\"],\n",
    "            \"3200 x 2000\": [\"3200 x 2000\", \"qhd+\", \"3k2k\", \"wqxga 16 10\"],\n",
    "            \"2880 x 1620\": [\"2880 x 1620\", \"qhd+ 16 9\", \"16 9 qhd+\", \"3k2k 16 9\"],\n",
    "            \"3456 x 2160\": [\"3456 x 2160\", \"retina 16\", \"16 inch retina\", \"3.5k\"],\n",
    "            \"2400 x 1600\": [\"2400 x 1600\", \"qxga+\"],\n",
    "        }\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        for key, synonyms in resolution_dict.items():\n",
    "            for syn in synonyms:\n",
    "                score = fuzz.partial_ratio(comp, syn)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = key\n",
    "        return best_match if best_score >= 50 else None\n",
    "\n",
    "    def process_refresh_rate(self, detected_rr: str) -> str:\n",
    "        \"\"\"\n",
    "        Lấy số từ refresh rate.\n",
    "        \"\"\"\n",
    "        if not detected_rr:\n",
    "            return None\n",
    "        match = re.search(r'(\\d+)', detected_rr)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def process_screen_size(self, detected_ss: str) -> float:\n",
    "        \"\"\"\n",
    "        Loại bỏ \"inch\" và chuyển thành float.\n",
    "        \"\"\"\n",
    "        if not detected_ss:\n",
    "            return None\n",
    "        comp = detected_ss.lower().replace(\"inch\", \"\").strip()\n",
    "        try:\n",
    "            return float(comp)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def postprocess(self, detected_components: dict, new_question: str) -> dict:\n",
    "        \"\"\"\n",
    "        Tổng hợp quá trình hậu xử lý:\n",
    "          - Áp dụng các hàm process_gpu, process_cpu, process_resolution, process_refresh_rate, process_screen_size.\n",
    "          - Nếu không tìm được phù hợp, trả về None cho component đó.\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        output[\"brand\"] = detected_components.get(\"brand\")\n",
    "        output[\"gpu\"] = self.process_gpu(detected_components.get(\"gpu\"), new_question)\n",
    "        output[\"cpu\"] = self.process_cpu(detected_components.get(\"cpu\"), new_question)\n",
    "        output[\"ram\"] = detected_components.get(\"ram\")  \n",
    "        output[\"resolution\"] = self.process_resolution(detected_components.get(\"resolution\"))\n",
    "        output[\"refresh rate\"] = self.process_refresh_rate(detected_components.get(\"refresh rate\"))\n",
    "        dt = detected_components.get(\"display type\")\n",
    "        output[\"display type\"] = dt.lower().strip() if dt else None\n",
    "        output[\"screen size\"] = self.process_screen_size(detected_components.get(\"screen size\"))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Components: {'brand': 'asus', 'gpu': 'geforce rtx 4070', 'cpu': 'intel core i7 12th', 'ram': 'ram 16gb', 'resolution': '2560 x 1600', 'refresh rate': None, 'display type': 'lcd', 'screen size': None}\n"
     ]
    }
   ],
   "source": [
    "extractor = ComponentExtractor(threshold=0.35, fuzzy_threshold=60)\n",
    "new_question = \"give me a laptop asus have rtx 4070, intel core i7 13th and ram 16gb , 512gb ssd\"\n",
    "detected = extractor.extract_components(new_question)\n",
    "print(\"Detected Components:\", detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Processed Components: {'brand': 'asus', 'gpu': ['nvidia geforce rtx 4070'], 'cpu': ['intel core i7 12700h', 'intel core i7 13700h', 'intel core i7 13700hx', 'intel core i7 12650h', 'intel core i7 11800h', 'intel core i7 13650hx', 'intel core i7 10870h', 'intel core i7 13620h', 'intel core i7 14650hx', 'intel core i7 14700hx', 'intel core i7 9750h', 'intel core i7 12800hx', 'intel core i7 1355u', 'intel core i7 1365u', 'intel core i7 1165g7', 'intel core i7 11850h', 'intel core i7 13800h', 'intel core i7 1260p', 'intel core i7 1360p', 'intel core i7 10610u', 'intel core i7 12800h', 'intel core i7 10750h', 'intel core i7 10875h', 'intel core i7 1185g7'], 'ram': 'ram 16gb', 'resolution': '2560 x 1600', 'refresh rate': None, 'display type': 'lcd', 'screen size': None}\n"
     ]
    }
   ],
   "source": [
    "post_processor = PostProcessor()\n",
    "final_components = post_processor.postprocess(detected, new_question)\n",
    "print(\"Final Processed Components:\", final_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astorine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
