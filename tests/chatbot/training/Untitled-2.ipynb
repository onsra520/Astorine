{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f6ecfdc4ac0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, json5, glob\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.qgene import generate_text\n",
    "\n",
    "paths = {\n",
    "    \"processed\": os.path.abspath(f\"{project_root}/data/storage/processed\"),\n",
    "    \"qfragments\": os.path.abspath(f\"{project_root}/intents/qfragments.json\"),\n",
    "    \"questions\": os.path.abspath(f\"{project_root}/intents/questions.csv\"),\n",
    "    \"qtrain\": os.path.abspath(f\"{project_root}/intents/qtrain.csv\"),\n",
    "    \"models\": os.path.abspath(f\"{project_root}/models/t5-small\"),\n",
    "    \"results\": os.path.abspath(f\"{project_root}/training/results\"),\n",
    "}\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "tf.device('/device:GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(datanum: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sinh dữ liệu từ generate_text với choice=\"s1\", \n",
    "    kết hợp dữ liệu cũ (nếu có) và lưu vào file qtrain.csv.\n",
    "    \"\"\"\n",
    "    if os.path.exists(paths[\"qtrain\"]):\n",
    "        old_data = pd.read_csv(paths[\"qtrain\"])\n",
    "        new_questions = pd.concat([old_data, generate_text(datanum, choice=\"s1\")])\n",
    "    else:\n",
    "        new_questions = generate_text(datanum, choice=\"s1\")\n",
    "    new_questions.to_csv(paths[\"qtrain\"], index=False)\n",
    "    return new_questions\n",
    "\n",
    "def load_dataset(num_samples=10000, batch_size=32, shuffle_buffer=1000, split_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Sinh dữ liệu bằng hàm dataset, chuyển đổi sang tf.data.Dataset ở dạng raw,\n",
    "    tách dữ liệu thành train và validation trước khi batching.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Số mẫu dữ liệu cần sinh.\n",
    "        batch_size: Kích thước batch.\n",
    "        shuffle_buffer: Kích thước buffer cho việc shuffle.\n",
    "        split_ratio: Tỷ lệ dữ liệu dùng cho validation (mặc định 0.1).\n",
    "        \n",
    "    Returns:\n",
    "        Một tuple (train_dataset, val_dataset), mỗi dataset đã được batch và prefetch.\n",
    "    \"\"\"\n",
    "    df = dataset(num_samples)\n",
    "    \n",
    "    # Tạo cột input: thêm \"Extract component: \" vào câu hỏi\n",
    "    inputs = df[\"question\"].apply(lambda x: \"Extract component: \" + x).tolist()\n",
    "    \n",
    "    # Tạo cột target: chuỗi chứa các component theo định dạng cố định\n",
    "    targets = df.apply(\n",
    "        lambda row: (\n",
    "            f\"BRAND: {row['brand']}; \"\n",
    "            f\"PRICE: {row['price']}; \"\n",
    "            f\"RAM: {row['ram']}; \"\n",
    "            f\"GPU: {row['gpu']}; \"\n",
    "            f\"CPU: {row['cpu']}; \"\n",
    "            f\"DISPLAY: {row['display']}; \"\n",
    "            f\"REFRESH_RATE: {row['refresh rate']}\"\n",
    "        ).strip(),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    # Tạo raw dataset chưa batch\n",
    "    raw_dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "    raw_dataset = raw_dataset.shuffle(shuffle_buffer)\n",
    "    \n",
    "    total_samples = len(inputs)\n",
    "    val_size = int(total_samples * split_ratio)\n",
    "    \n",
    "    # Tách dữ liệu: Lấy val_size mẫu cho validation, phần còn lại cho training\n",
    "    val_dataset = raw_dataset.take(val_size)\n",
    "    train_dataset = raw_dataset.skip(val_size)\n",
    "    \n",
    "    # Sau đó, áp dụng batch và prefetch cho cả train và validation\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "Input mẫu: b'Extract component: Assistance in finding laptops engineered by lenovo, equipped with intel core i7 9750h besides nvidia rtx a3000 plus ram 32gb in addition to 3k2k display panel alongside 300hz ideal for running machine learning frameworks above 1688 USD.'\n",
      "Target mẫu: b'BRAND: lenovo; PRICE: above 1688 USD; RAM: ram 32gb; GPU: nvidia rtx a3000; CPU: intel core i7 9750h; DISPLAY: 3k2k display panel; REFRESH_RATE: 300hz'\n",
      "\n",
      "Validation dataset:\n",
      "Input mẫu: b'Extract component: Show me some laptops crafted by dell, that have ryzen ai 7 & amd radeon rx 6800s coupled with 24gb ram as well as display resolution 4k accompanied by 240hz suitable for running high-performance data modeling software in the region of $2972.'\n",
      "Target mẫu: b'BRAND: dell; PRICE: in the region of $2972; RAM: 24gb ram; GPU: amd radeon rx 6800s; CPU: ryzen ai 7; DISPLAY: display resolution 4k; REFRESH_RATE: 240hz'\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = load_dataset(num_samples=100, batch_size=4)\n",
    "print(\"Train dataset:\")\n",
    "for inp, tgt in train_ds.take(1):\n",
    "    print(\"Input mẫu:\", inp.numpy()[0])\n",
    "    print(\"Target mẫu:\", tgt.numpy()[0])\n",
    "print(\"\\nValidation dataset:\")\n",
    "for inp, tgt in val_ds.take(1):\n",
    "    print(\"Input mẫu:\", inp.numpy()[0])\n",
    "    print(\"Target mẫu:\", tgt.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onsra/miniconda3/envs/Astorine/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước sau projection: (10, 512)\n"
     ]
    }
   ],
   "source": [
    "def build_projection_layer(input_dim=256, output_dim=512):\n",
    "    \"\"\"\n",
    "    Xây dựng một projection layer dùng tf.keras.layers.Dense chuyển từ 256 lên 512.\n",
    "    \"\"\"\n",
    "    projection_layer = tf.keras.layers.Dense(output_dim, input_shape=(input_dim,))\n",
    "    return projection_layer\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_matrix = np.random.randn(vocab_size, 256).astype(\"float32\")\n",
    "projection = build_projection_layer(256, 512)\n",
    "projected = projection(embedding_matrix)\n",
    "print(\"Kích thước sau projection:\", projected.shape)  # (10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_custom_embeddings(embedding_matrix, projection_layer, t5_model, union_vocab):\n",
    "    \"\"\"\n",
    "    Cập nhật embedding layer của T5 với custom embeddings sau khi qua projection layer.\n",
    "    \n",
    "    Args:\n",
    "        embedding_matrix: numpy array (vocab_size, 256)\n",
    "        projection_layer: tf.keras.layers.Dense chuyển từ 256 -> 512\n",
    "        t5_model: mô hình TFT5ForConditionalGeneration\n",
    "        union_vocab: dict mapping token -> index (vocabulary)\n",
    "    \n",
    "    Returns:\n",
    "        t5_model với embedding đã được cập nhật.\n",
    "    \"\"\"\n",
    "    # Resize embedding của T5 cho phù hợp với union_vocab mới.\n",
    "    new_vocab_size = len(union_vocab)\n",
    "    t5_model.resize_token_embeddings(new_vocab_size)\n",
    "    \n",
    "    # Chuyển đổi embedding_matrix sang tensor và qua projection layer.\n",
    "    custom_embeddings = tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)  # (vocab_size, 256)\n",
    "    projected_embeddings = projection_layer(custom_embeddings)  # (vocab_size, 512)\n",
    "    \n",
    "    # Cập nhật embedding layer (shared) của T5 sử dụng set_weights.\n",
    "    # Lưu ý: set_weights cần nhận vào danh sách các mảng numpy.\n",
    "    t5_model.shared.set_weights([projected_embeddings.numpy()])\n",
    "    \n",
    "    # Đảm bảo embedding layer được fine-tune (trainable).\n",
    "    t5_model.shared.trainable = True\n",
    "    return t5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số callback: 2\n"
     ]
    }
   ],
   "source": [
    "def get_checkpoint_name():\n",
    "    \"\"\"\n",
    "    Trả về tên checkpoint theo định dạng: checkpoint-{HHMM}-{DDMMYYYY}\n",
    "    Giờ ở định dạng 24h.\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    checkpoint_name = f\"checkpoint-{now.strftime('%H%M')}-{now.strftime('%d%m%Y')}\"\n",
    "    return checkpoint_name\n",
    "\n",
    "class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback lưu model ở định dạng SavedModel sau mỗi epoch và\n",
    "    tự động xóa các checkpoint cũ nếu số lượng vượt quá 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor='val_loss', mode='min', early_stop=5):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.early_stop = early_stop  # Nếu early_stop là False thì tắt EarlyStopping, nếu int thì dùng giá trị đó.\n",
    "        self.checkpoints_dir = paths[\"results\"]\n",
    "        os.makedirs(self.checkpoints_dir, exist_ok=True)\n",
    "        self.checkpoints = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Lưu model sau mỗi epoch.\n",
    "        checkpoint_name = get_checkpoint_name()\n",
    "        save_path = os.path.join(self.checkpoints_dir, f\"{checkpoint_name}\")\n",
    "        self.model.save(save_path, save_format=\"tf\")\n",
    "        self.checkpoints.append(save_path)\n",
    "        print(f\"Đã lưu checkpoint: {save_path}\")\n",
    "        self._cleanup_checkpoints()\n",
    "    \n",
    "    def _cleanup_checkpoints(self):\n",
    "        # Xóa các checkpoint cũ nếu số lượng vượt quá 3.\n",
    "        if len(self.checkpoints) > 3:\n",
    "            num_to_delete = len(self.checkpoints) - 3\n",
    "            for i in range(num_to_delete):\n",
    "                checkpoint_to_delete = self.checkpoints.pop(0)\n",
    "                # Xóa thư mục checkpoint\n",
    "                tf.io.gfile.rmtree(checkpoint_to_delete)\n",
    "                print(f\"Đã xóa checkpoint cũ: {checkpoint_to_delete}\")\n",
    "\n",
    "def get_callbacks(monitor='val_loss', mode='min', early_stop=True):\n",
    "    \"\"\"\n",
    "    Trả về danh sách các callback cho việc huấn luyện.\n",
    "    early_stop: Nếu là True (hoặc số int), sử dụng EarlyStopping với patience=5 (hoặc giá trị int đó).\n",
    "                Nếu là False, không sử dụng EarlyStopping.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    # Callback lưu checkpoint\n",
    "    checkpoint_cb = CustomModelCheckpoint(monitor=monitor, mode=mode, early_stop=5 if early_stop is True else early_stop)\n",
    "    callbacks.append(checkpoint_cb)\n",
    "    \n",
    "    # Callback EarlyStopping\n",
    "    if early_stop:\n",
    "        patience = 5 if early_stop is True else early_stop\n",
    "        early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor=monitor, mode=mode, patience=patience)\n",
    "        callbacks.append(early_stop_cb)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "\n",
    "cbs = get_callbacks()\n",
    "print(\"Số callback:\", len(cbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "global t5_tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tf_tokenize_map(input_text, target_text):\n",
    "    input_ids, attention_mask, labels = tf.py_function(\n",
    "        func=tokenize_function,\n",
    "        inp=[input_text, target_text],\n",
    "        Tout=(tf.int32, tf.int32, tf.int32)\n",
    "    )\n",
    "    input_ids.set_shape([None, 512])\n",
    "    attention_mask.set_shape([None, 512])\n",
    "    labels.set_shape([None, 128])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "def tokenize_function(input_text, target_text):\n",
    "    # Chuyển đổi tensor thành danh sách (list)\n",
    "    input_list = input_text.numpy().tolist()\n",
    "    target_list = target_text.numpy().tolist()\n",
    "    \n",
    "    # Giải mã từng phần tử nếu cần (nếu là bytes)\n",
    "    input_list = [s.decode('utf-8') if isinstance(s, bytes) else s for s in input_list]\n",
    "    target_list = [s.decode('utf-8') if isinstance(s, bytes) else s for s in target_list]\n",
    "    \n",
    "    tokenized_inputs = t5_tokenizer(\n",
    "        input_list,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    tokenized_targets = t5_tokenizer(\n",
    "        target_list,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    return tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], tokenized_targets['input_ids']\n",
    "\n",
    "def custom_train_loop(model, train_dataset, val_dataset, optimizer, num_epochs=10, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Huấn luyện mô hình với gradient accumulation.\n",
    "    \n",
    "    Args:\n",
    "      model: mô hình T5 đã tích hợp custom embeddings.\n",
    "      train_dataset: tf.data.Dataset đã token hóa cho training.\n",
    "      val_dataset: tf.data.Dataset đã token hóa cho validation.\n",
    "      optimizer: Optimizer (ví dụ, Adam).\n",
    "      num_epochs: Số epoch huấn luyện.\n",
    "      accumulation_steps: Số bước tích lũy gradient trước khi cập nhật trọng số.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        # Khởi tạo danh sách tích lũy gradient cho từng biến trainable\n",
    "        accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "        \n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            # Mỗi batch đã ở dạng dictionary: {'input_ids':..., 'attention_mask':..., 'labels':...}\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'], \n",
    "                    attention_mask=batch['attention_mask'], \n",
    "                    labels=batch['labels'], \n",
    "                    training=True\n",
    "                )\n",
    "                # Chia loss cho số bước tích lũy để cân bằng\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            # Tích lũy gradient\n",
    "            accumulated_gradients = [accum_grad + grad for accum_grad, grad in zip(accumulated_gradients, gradients)]\n",
    "            epoch_loss += outputs.loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
    "                # Reset accumulated gradients\n",
    "                accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "                print(f\"Step {step+1}: Loss = {(loss * accumulation_steps).numpy():.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1} Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation sau mỗi epoch\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        for batch in val_dataset:\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'], \n",
    "                attention_mask=batch['attention_mask'], \n",
    "                labels=batch['labels'], \n",
    "                training=False\n",
    "            )\n",
    "            val_loss += outputs.loss\n",
    "            val_batches += 1\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # 1. Load raw dataset: train and validation\n",
    "    train_raw, val_raw = load_dataset(num_samples=10000, batch_size=32, split_ratio=0.1)\n",
    "    \n",
    "    # 2. Tải tokenizer T5\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    \n",
    "    # 3. Xây dựng union vocabulary (ở đây dùng vocabulary của tokenizer T5)\n",
    "    union_vocab = t5_tokenizer.get_vocab()\n",
    "    print(\"Vocabulary size:\", len(union_vocab))\n",
    "    \n",
    "    # 4. Tạo embedding matrix giả lập (với shape: vocab_size x 256)\n",
    "    vocab_size = len(union_vocab)\n",
    "    embedding_matrix = np.random.randn(vocab_size, 256).astype(\"float32\")\n",
    "    \n",
    "    # 5. Xây dựng projection layer (256 -> 512)\n",
    "    projection_layer = build_projection_layer(input_dim=256, output_dim=512)\n",
    "    \n",
    "    # 6. Tải mô hình T5 phiên bản TensorFlow\n",
    "    t5_model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    \n",
    "    # 7. Tích hợp custom embeddings vào T5\n",
    "    t5_model = integrate_custom_embeddings(embedding_matrix, projection_layer, t5_model, union_vocab)\n",
    "    print(\"Custom embeddings đã được tích hợp vào T5.\")\n",
    "    \n",
    "    # 8. Áp dụng hàm token hóa cho train và validation dataset\n",
    "    train_dataset = train_raw.map(tf_tokenize_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_raw.map(tf_tokenize_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 9. Thiết lập optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    \n",
    "    # 10. Chạy custom training loop với gradient accumulation\n",
    "    custom_train_loop(t5_model, train_dataset, val_dataset, optimizer, num_epochs=10, accumulation_steps=4)\n",
    "    \n",
    "    # 11. Ví dụ inference: Lấy một batch từ val_dataset và generate output\n",
    "    for batch in val_dataset.take(1):\n",
    "        outputs = t5_model.generate(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            max_length=128\n",
    "        )\n",
    "        decoded_outputs = t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        print(\"Inference Output mẫu:\", decoded_outputs)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astorine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
