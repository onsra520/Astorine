{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, shutil, warnings\n",
    "import json5\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.qgene import generate_text\n",
    "\n",
    "# from nlp.nlp_model import NLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"processed\": os.path.abspath(f\"{project_root}/data/storage/processed\"),\n",
    "    \"qfragments\": os.path.abspath(f\"{project_root}/intents/qfragments.json\"),\n",
    "    \"questions\": os.path.abspath(f\"{project_root}/intents/questions.csv\"),\n",
    "    \"trained_questions\": os.path.abspath(f\"{project_root}/intents/trained_questions.csv\"),\n",
    "    \"models\": os.path.abspath(f\"{project_root}/models/t5-small\"),\n",
    "    \"results\": os.path.abspath(f\"{project_root}/training/results\"),\n",
    "}\n",
    "\n",
    "os.makedirs(paths[\"models\"], exist_ok=True) \n",
    "os.makedirs(paths[\"results\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerSaver(TrainerCallback):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        ckpt_dir = Path(args.output_dir) / f\"checkpoint-{state.global_step}\"\n",
    "        self.tokenizer.save_pretrained(ckpt_dir)\n",
    "        return control\n",
    "\n",
    "class NLPModel:\n",
    "    def __init__(self, datanum: int = 1000) -> None:\n",
    "        # Load data\n",
    "        self.qfragments = json5.load(open(paths[\"qfragments\"]))\n",
    "        self.questions = self._dataset(datanum)\n",
    "\n",
    "        # Model setup\n",
    "        self.model_checkpoint = self._load_checkpoint()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_checkpoint)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_checkpoint)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)\n",
    "\n",
    "        # Initialize datasets\n",
    "        self._prepare_datasets()\n",
    "\n",
    "    def _dataset(self, datanum: int = 1000) -> pd.DataFrame:\n",
    "        if os.path.exists(paths[\"trained_questions\"]):\n",
    "            old_data = pd.read_csv(paths[\"trained_questions\"])\n",
    "            new_questions = pd.concat([old_data, generate_text(datanum)])\n",
    "        else:\n",
    "            new_questions = generate_text(datanum)\n",
    "        \n",
    "        new_questions.to_csv(paths[\"trained_questions\"], index=False)\n",
    "        return new_questions\n",
    "    \n",
    "    def _rows_preprocessor(self, row: pd.Series) -> dict:\n",
    "        return {\n",
    "        \"input_text\": f\"{row['question']}\",\n",
    "        \"target_text\": f\"\"\"\n",
    "            BRAND: {row['brand']};  \n",
    "            PRICE: {row['price']}                   \n",
    "            RAM: {row['ram']}; \n",
    "            GPU: {row['gpu']}; \n",
    "            CPU: {row['cpu']}; \n",
    "            DISPLAY: {row['display']};             \n",
    "            REFRESH_RATE: {row['refresh rate']}; \n",
    "            \"\"\",\n",
    "    }\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        tokenized = self.tokenizer(\n",
    "            examples[\"input_text\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                examples[\"target_text\"],\n",
    "                max_length=128,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "        tokenized[\"labels\"] = labels[\"input_ids\"]\n",
    "        return tokenized\n",
    "\n",
    "    def _prepare_datasets(self):\n",
    "        processed_data = self.questions.apply(self._rows_preprocessor, axis=1)\n",
    "        df_processed = pd.DataFrame(list(processed_data))\n",
    "        full_dataset = Dataset.from_pandas(df_processed)\n",
    "        split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        self.tokenized_train = split_datasets[\"train\"].map(\n",
    "            self._tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=split_datasets[\"train\"].column_names,\n",
    "        )\n",
    "\n",
    "        self.tokenized_eval = split_datasets[\"test\"].map(\n",
    "            self._tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=split_datasets[\"test\"].column_names,\n",
    "        )\n",
    "\n",
    "    def _load_checkpoint(self, resume_checkpoint: str = \"auto\") -> str:\n",
    "        def is_valid_checkpoint(ckpt_path: Path) -> bool:\n",
    "            tokenizer_files = {\n",
    "                \"tokenizer_config.json\",\n",
    "                \"special_tokens_map.json\",\n",
    "                \"spiece.model\"\n",
    "            }\n",
    "\n",
    "            model_files = {\n",
    "                \"config.json\",\n",
    "                \"pytorch_model.bin\",      \n",
    "                \"model.safetensors\",      \n",
    "                \"generation_config.json\"    \n",
    "            }\n",
    "            \n",
    "            has_model_files = any((ckpt_path / f).exists() for f in model_files if f != \"config.json\")\n",
    "            has_config = (ckpt_path / \"config.json\").exists()\n",
    "\n",
    "            has_tokenizer = all((ckpt_path / f).exists() for f in tokenizer_files)\n",
    "            \n",
    "            return has_config and has_model_files and has_tokenizer\n",
    "        if resume_checkpoint == \"auto\":\n",
    "            checkpoint_dirs = sorted(\n",
    "                Path(paths[\"results\"]).glob(\"checkpoint-*\"), \n",
    "                key=lambda x: int(x.name.split(\"-\")[-1]) if x.name.split(\"-\")[-1].isdigit() else 0,\n",
    "                reverse=True\n",
    "            )\n",
    "            for ckpt_dir in checkpoint_dirs:\n",
    "                if ckpt_dir.is_dir() and is_valid_checkpoint(ckpt_dir):\n",
    "                    print(f\"Auto-selected valid checkpoint: {ckpt_dir}\")\n",
    "                    return str(ckpt_dir)\n",
    "            \n",
    "            print(\"No valid checkpoints found in auto mode, using default t5-small\")\n",
    "            return \"t5-small\"\n",
    "\n",
    "        manual_ckpt = Path(resume_checkpoint)\n",
    "        if manual_ckpt.exists():\n",
    "            if manual_ckpt.is_dir() and is_valid_checkpoint(manual_ckpt):\n",
    "                return str(manual_ckpt)\n",
    "            \n",
    "            print(f\"Manual checkpoint {manual_ckpt} is invalid, checking in results directory...\")\n",
    "            manual_ckpt = Path(paths[\"results\"]) / resume_checkpoint\n",
    "            if manual_ckpt.exists() and manual_ckpt.is_dir() and is_valid_checkpoint(manual_ckpt):\n",
    "                return str(manual_ckpt)\n",
    "\n",
    "        print(f\"Checkpoint {resume_checkpoint} is invalid or corrupted. Reverting to t5-small\")\n",
    "        return \"t5-small\"\n",
    "\n",
    "    def _del_checkpoint(self) -> None:\n",
    "        checkpoint_dirs = list(Path(paths[\"results\"]).glob(\"checkpoint-*\"))\n",
    "        checkpoints = sorted(\n",
    "            [\n",
    "                ckpt\n",
    "                for ckpt in checkpoint_dirs\n",
    "                if ckpt.is_dir() and ckpt.name.split(\"-\")[-1].isdigit()\n",
    "            ],\n",
    "            key=lambda x: int(x.name.split(\"-\")[-1]),\n",
    "        )\n",
    "        if len(checkpoints) > 3:\n",
    "            for checkpoint in checkpoints[:-3]:\n",
    "                shutil.rmtree(checkpoint)\n",
    "                print(f\"Removed old checkpoint: {checkpoint}\")\n",
    "\n",
    "\n",
    "    def _del_checkpoint(self) -> None:\n",
    "        checkpoint_dirs = list(Path(paths[\"results\"]).glob(\"checkpoint-*\"))\n",
    "        checkpoints = sorted(\n",
    "            [\n",
    "                ckpt\n",
    "                for ckpt in checkpoint_dirs\n",
    "                if ckpt.is_dir() and ckpt.name.split(\"-\")[-1].isdigit()\n",
    "            ],\n",
    "            key=lambda x: int(x.name.split(\"-\")[-1]),\n",
    "        )\n",
    "        if len(checkpoints) > 3:\n",
    "            for checkpoint in checkpoints[:-3]:\n",
    "                shutil.rmtree(checkpoint)\n",
    "                print(f\"Removed old checkpoint: {checkpoint}\")\n",
    "\n",
    "    def _nlptraining(\n",
    "        self,\n",
    "        early_stopping_patience: int = 3,\n",
    "        resume_checkpoint: str = None,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 3e-5,\n",
    "        num_train_epochs: int = 1,\n",
    "    ) -> None:\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(paths[\"results\"]),\n",
    "            overwrite_output_dir=True,\n",
    "            report_to=\"none\",          \n",
    "            logging_strategy=\"steps\", \n",
    "            evaluation_strategy=\"steps\",   \n",
    "            save_strategy=\"steps\",                           \n",
    "            logging_steps=25,              \n",
    "            eval_steps=25,                 \n",
    "            save_steps=25,           \n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=0.01,\n",
    "            gradient_accumulation_steps=2,\n",
    "            fp16=True,\n",
    "            load_best_model_at_end=True,\n",
    "            greater_is_better=True,\n",
    "            eval_accumulation_steps=1,\n",
    "            resume_from_checkpoint=resume_checkpoint,\n",
    "            save_safetensors=False,\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_train,\n",
    "            eval_dataset=self.tokenized_eval,\n",
    "            data_collator=self.data_collator,\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
    "                TokenizerSaver(self.tokenizer),  \n",
    "            ],\n",
    "        )\n",
    "            \n",
    "        self.trainer.train()\n",
    "        self.model.save_pretrained(paths[\"models\"])\n",
    "        self.tokenizer.save_pretrained(paths[\"models\"])\n",
    "\n",
    "model = NLPModel(1000)\n",
    "model._nlptraining(\n",
    "    resume_checkpoint = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os, shutil, warnings, re\n",
    "# import json5\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import (\n",
    "#     T5Tokenizer,\n",
    "#     T5ForConditionalGeneration,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     EarlyStoppingCallback,\n",
    "#     TrainerCallback\n",
    "# )\n",
    "# import torch\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# project_root = Path(__file__).resolve().parents[1]\n",
    "# sys.path.append(str(project_root))\n",
    "\n",
    "# from utils.qgene import generate_text\n",
    "\n",
    "# paths = {\n",
    "#     \"processed\": os.path.abspath(f\"{project_root}/data/storage/processed\"),\n",
    "#     \"qfragments\": os.path.abspath(f\"{project_root}/intents/qfragments.json\"),\n",
    "#     \"questions\": os.path.abspath(f\"{project_root}/intents/questions.csv\"),\n",
    "#     \"trained_questions\": os.path.abspath(f\"{project_root}/intents/trained_questions.csv\"),\n",
    "#     \"models\": os.path.abspath(f\"{project_root}/models/t5-small\"),\n",
    "#     \"results\": os.path.abspath(f\"{project_root}/training/results\"),\n",
    "# }\n",
    "\n",
    "# os.makedirs(paths[\"models\"], exist_ok=True) \n",
    "# os.makedirs(paths[\"results\"], exist_ok=True)\n",
    "\n",
    "# class TokenizerSaver(TrainerCallback):\n",
    "#     def __init__(self, tokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def on_save(self, args, state, control, **kwargs):\n",
    "#         ckpt_dir = Path(args.output_dir) / f\"checkpoint-{state.global_step}\"\n",
    "#         self.tokenizer.save_pretrained(ckpt_dir)\n",
    "#         return control\n",
    "\n",
    "# class NLPModel:\n",
    "#     def __init__(self, datanum: int = 1000, mode: str = \"train\") -> None:\n",
    "#         \"\"\"\n",
    "#         Nếu mode == \"train\": tạo dataset, load checkpoint (nếu có) và thiết lập model để huấn luyện.\n",
    "#         Nếu mode == \"usage\": load model đã lưu từ paths[\"models\"] và chỉ sử dụng hàm inference.\n",
    "#         \"\"\"\n",
    "#         self.mode = mode\n",
    "#         # Xác định device (GPU nếu có)\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "#         if self.mode == \"train\":\n",
    "#             # Load dữ liệu\n",
    "#             self.qfragments = json5.load(open(paths[\"qfragments\"]))\n",
    "#             self.questions = self._dataset(datanum)\n",
    "            \n",
    "#             # Huấn luyện: load checkpoint (từ checkpoint đã lưu hay revert về t5-small)\n",
    "#             self.model_checkpoint = self._loading(resume_checkpoint=\"auto\")\n",
    "#             self.tokenizer = T5Tokenizer.from_pretrained(self.model_checkpoint)\n",
    "#             self.model = T5ForConditionalGeneration.from_pretrained(self.model_checkpoint)\n",
    "#             self.model.to(self.device)\n",
    "#             if self.model_checkpoint != \"t5-small\":\n",
    "#                 self.checkpoint_used = self.model_checkpoint\n",
    "#                 print(f\"Tiếp tục huấn luyện từ checkpoint: {self.checkpoint_used}\")\n",
    "#             else:\n",
    "#                 self.checkpoint_used = None\n",
    "#                 print(\"Huấn luyện từ đầu (không có checkpoint hợp lệ)\")\n",
    "#         elif self.mode == \"usage\":\n",
    "#             # Chế độ sử dụng: load model và tokenizer đã lưu\n",
    "#             self.tokenizer = T5Tokenizer.from_pretrained(paths[\"models\"])\n",
    "#             self.model = T5ForConditionalGeneration.from_pretrained(paths[\"models\"])\n",
    "#             self.model.to(self.device)\n",
    "#             print(\"Đang sử dụng model đã lưu.\")\n",
    "#         else:\n",
    "#             raise ValueError(\"Mode phải là 'train' hoặc 'usage'.\")\n",
    "\n",
    "#         # Các bước sau chỉ áp dụng cho chế độ train\n",
    "#         if self.mode == \"train\":\n",
    "#             self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)\n",
    "#             self._prepare_datasets()\n",
    "            \n",
    "#         # Thêm các token đặc biệt (áp dụng cho cả 2 mode)\n",
    "#         special_tokens = [\"BRAND:\", \"PRICE:\", \"RAM:\", \"GPU:\", \"CPU:\", \"DISPLAY:\", \"REFRESH_RATE:\", \";\", \"none\"]\n",
    "#         self.tokenizer.add_tokens(special_tokens)\n",
    "#         self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "#     def _dataset(self, datanum: int = 1000) -> pd.DataFrame:\n",
    "#         if os.path.exists(paths[\"trained_questions\"]):\n",
    "#             old_data = pd.read_csv(paths[\"trained_questions\"])\n",
    "#             new_questions = pd.concat([old_data, generate_text(datanum)])\n",
    "#         else:\n",
    "#             new_questions = generate_text(datanum)\n",
    "        \n",
    "#         new_questions.to_csv(paths[\"trained_questions\"], index=False)\n",
    "#         return new_questions\n",
    "    \n",
    "#     def _rows_preprocessor(self, row: pd.Series) -> dict:\n",
    "#         return {\n",
    "#             \"input_text\": f\"Extract component: {row['question']}\",\n",
    "#             \"target_text\": (\n",
    "#                 f\"BRAND: {row['brand']}; \"\n",
    "#                 f\"PRICE: {row['price']}; \"\n",
    "#                 f\"RAM: {row['ram']}; \"\n",
    "#                 f\"GPU: {row['gpu']}; \"\n",
    "#                 f\"CPU: {row['cpu']}; \"\n",
    "#                 f\"DISPLAY: {row['display']}; \"\n",
    "#                 f\"REFRESH_RATE: {row['refresh rate']}\"\n",
    "#             ).replace(\"  \", \" \").strip(),\n",
    "#         }\n",
    "\n",
    "#     def _tokenize_function(self, examples):\n",
    "#         tokenized = self.tokenizer(\n",
    "#             examples[\"input_text\"],\n",
    "#             max_length=512,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#         )\n",
    "#         with self.tokenizer.as_target_tokenizer():\n",
    "#             labels = self.tokenizer(\n",
    "#                 examples[\"target_text\"],\n",
    "#                 max_length=128,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#             )\n",
    "#         tokenized[\"labels\"] = labels[\"input_ids\"]\n",
    "#         return tokenized\n",
    "\n",
    "#     def _prepare_datasets(self):\n",
    "#         processed_data = self.questions.apply(self._rows_preprocessor, axis=1)\n",
    "#         df_processed = pd.DataFrame(list(processed_data))\n",
    "#         full_dataset = Dataset.from_pandas(df_processed)\n",
    "#         split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "#         self.tokenized_train = split_datasets[\"train\"].map(\n",
    "#             self._tokenize_function,\n",
    "#             batched=True,\n",
    "#             remove_columns=split_datasets[\"train\"].column_names,\n",
    "#         )\n",
    "#         self.tokenized_eval = split_datasets[\"test\"].map(\n",
    "#             self._tokenize_function,\n",
    "#             batched=True,\n",
    "#             remove_columns=split_datasets[\"test\"].column_names,\n",
    "#         )\n",
    "\n",
    "#     def _loading(self, resume_checkpoint: str = \"auto\") -> str:\n",
    "#         \"\"\"\n",
    "#         Hàm _loading tương tự như _load_checkpoint cũ, dùng để kiểm tra và load checkpoint hợp lệ.\n",
    "#         \"\"\"\n",
    "#         def is_valid_checkpoint(ckpt_path: Path) -> bool:\n",
    "#             tokenizer_files = {\n",
    "#                 \"tokenizer_config.json\",\n",
    "#                 \"special_tokens_map.json\",\n",
    "#                 \"spiece.model\"\n",
    "#             }\n",
    "#             model_files = {\n",
    "#                 \"config.json\",\n",
    "#                 \"pytorch_model.bin\",      \n",
    "#                 \"model.safetensors\",      \n",
    "#                 \"generation_config.json\"    \n",
    "#             }\n",
    "#             has_model_files = any((ckpt_path / f).exists() for f in model_files if f != \"config.json\")\n",
    "#             has_config = (ckpt_path / \"config.json\").exists()\n",
    "#             has_tokenizer = all((ckpt_path / f).exists() for f in tokenizer_files)\n",
    "#             return has_config and has_model_files and has_tokenizer\n",
    "\n",
    "#         if resume_checkpoint == \"auto\":\n",
    "#             checkpoint_dirs = sorted(\n",
    "#                 Path(paths[\"results\"]).glob(\"checkpoint-*\"), \n",
    "#                 key=lambda x: int(x.name.split(\"-\")[-1]) if x.name.split(\"-\")[-1].isdigit() else 0,\n",
    "#                 reverse=True\n",
    "#             )\n",
    "#             for ckpt_dir in checkpoint_dirs:\n",
    "#                 if ckpt_dir.is_dir() and is_valid_checkpoint(ckpt_dir):\n",
    "#                     print(f\"Auto-selected valid checkpoint: {ckpt_dir}\")\n",
    "#                     return str(ckpt_dir)\n",
    "#             print(\"No valid checkpoints found in auto mode, using default t5-small\")\n",
    "#             return \"t5-small\"\n",
    "\n",
    "#         manual_ckpt = Path(resume_checkpoint)\n",
    "#         if manual_ckpt.exists():\n",
    "#             if manual_ckpt.is_dir() and is_valid_checkpoint(manual_ckpt):\n",
    "#                 return str(manual_ckpt)\n",
    "#             print(f\"Manual checkpoint {manual_ckpt} is invalid, checking in results directory...\")\n",
    "#             manual_ckpt = Path(paths[\"results\"]) / resume_checkpoint\n",
    "#             if manual_ckpt.exists() and manual_ckpt.is_dir() and is_valid_checkpoint(manual_ckpt):\n",
    "#                 return str(manual_ckpt)\n",
    "#         print(f\"Checkpoint {resume_checkpoint} is invalid or corrupted. Reverting to t5-small\")\n",
    "#         return \"t5-small\"\n",
    "\n",
    "#     def _del_checkpoint(self) -> None:\n",
    "#         checkpoint_dirs = list(Path(paths[\"results\"]).glob(\"checkpoint-*\"))\n",
    "#         checkpoints = sorted(\n",
    "#             [\n",
    "#                 ckpt\n",
    "#                 for ckpt in checkpoint_dirs\n",
    "#                 if ckpt.is_dir() and ckpt.name.split(\"-\")[-1].isdigit()\n",
    "#             ],\n",
    "#             key=lambda x: int(x.name.split(\"-\")[-1]),\n",
    "#         )\n",
    "#         if len(checkpoints) > 3:\n",
    "#             for checkpoint in checkpoints[:-3]:\n",
    "#                 shutil.rmtree(checkpoint)\n",
    "#                 # Không cần thông báo khi xóa checkpoint\n",
    "\n",
    "#     def _nlptraining(\n",
    "#         self,\n",
    "#         early_stopping_patience: int = 3,\n",
    "#         resume_checkpoint: str = None,\n",
    "#         batch_size: int = 16,\n",
    "#         learning_rate: float = 3e-5,\n",
    "#         num_train_epochs: int = 1,\n",
    "#     ) -> None:\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=str(paths[\"results\"]),\n",
    "#             overwrite_output_dir=True,\n",
    "#             report_to=\"none\",          \n",
    "#             logging_strategy=\"steps\", \n",
    "#             evaluation_strategy=\"steps\",   \n",
    "#             save_strategy=\"steps\",                           \n",
    "#             logging_steps=25,              \n",
    "#             eval_steps=25,                 \n",
    "#             save_steps=25,           \n",
    "#             learning_rate=learning_rate,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             num_train_epochs=num_train_epochs,\n",
    "#             weight_decay=0.01,\n",
    "#             gradient_accumulation_steps=2,\n",
    "#             fp16=True,\n",
    "#             load_best_model_at_end=True,\n",
    "#             greater_is_better=True,\n",
    "#             eval_accumulation_steps=1,\n",
    "#             resume_from_checkpoint=resume_checkpoint,\n",
    "#             save_safetensors=False,\n",
    "#         )\n",
    "\n",
    "#         self.trainer = Trainer(\n",
    "#             model=self.model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=self.tokenized_train,\n",
    "#             eval_dataset=self.tokenized_eval,\n",
    "#             data_collator=self.data_collator,\n",
    "#             callbacks=[\n",
    "#                 EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
    "#                 TokenizerSaver(self.tokenizer),  \n",
    "#             ],\n",
    "#         )\n",
    "            \n",
    "#         self.trainer.train()\n",
    "#         self.model.save_pretrained(paths[\"models\"])\n",
    "#         self.tokenizer.save_pretrained(paths[\"models\"])\n",
    "#         # Sau khi huấn luyện, xóa các checkpoint cũ nếu cần\n",
    "#         self._del_checkpoint()\n",
    "\n",
    "#     def inference(self, text: str) -> dict:\n",
    "#         \"\"\"\n",
    "#         Nhận vào 1 chuỗi văn bản và trả về dict chứa các component:\n",
    "#         { 'cpu': '...', 'gpu': '...', 'display': '...', 'ram': '...', 'brand': '...', 'price': '...', 'refresh_rate': '...' }\n",
    "#         Nếu một component không được phát hiện thì giá trị là None.\n",
    "#         \"\"\"\n",
    "#         self.model.eval()  # Đưa model vào chế độ eval\n",
    "#         input_text = \"Extract component: \" + text\n",
    "#         inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "#         inputs = inputs.to(self.device)  # Chuyển tensor đầu vào sang cùng device với model\n",
    "        \n",
    "#         outputs = self.model.generate(\n",
    "#             inputs,\n",
    "#             max_length=128,\n",
    "#             num_beams=5,\n",
    "#             early_stopping=True\n",
    "#         )\n",
    "#         output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "#         pattern = r\"([A-Z_]+):\\s*([^;]+)\"\n",
    "#         matches = re.findall(pattern, output_text)\n",
    "        \n",
    "#         components = [\"BRAND\", \"PRICE\", \"RAM\", \"GPU\", \"CPU\", \"DISPLAY\", \"REFRESH_RATE\"]\n",
    "#         result = { comp.lower(): None for comp in components }\n",
    "        \n",
    "#         for key, value in matches:\n",
    "#             key = key.strip().upper()\n",
    "#             value = value.strip() if value.strip() else None\n",
    "#             if key in components:\n",
    "#                 result[key.lower()] = value\n",
    "#         return result\n",
    "\n",
    "# # Ví dụ sử dụng cho chế độ train:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Chạy chế độ train:\n",
    "#     model_train = NLPModel(300, mode=\"train\")\n",
    "#     model_train._nlptraining(resume_checkpoint=\"auto\")\n",
    "    \n",
    "#     # Chạy chế độ usage (sau khi đã có model đã lưu)\n",
    "#     model_usage = NLPModel(mode=\"usage\")\n",
    "#     sample_text = \"recommend me a laptop have rtx 3070, ram 16GB, about 2000 USD\"\n",
    "#     inference_result = model_usage.inference(sample_text)\n",
    "#     print(\"Inference result:\", inference_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
