{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "import json5\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "#     \n",
    "#     T5ForConditionalGeneration,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     EarlyStoppingCallback,\n",
    "#     TrainerCallback\n",
    "# )\n",
    "# import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.qgene import generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"processed\": os.path.abspath(f\"{project_root}/data/storage/processed\"),\n",
    "    \"qfragments\": os.path.abspath(f\"{project_root}/intents/qfragments.json\"),\n",
    "    \"questions\": os.path.abspath(f\"{project_root}/intents/questions.csv\"),\n",
    "    \"qtrain\": os.path.abspath(f\"{project_root}/intents/qtrain.csv\"),\n",
    "    \"models\": os.path.abspath(f\"{project_root}/models/t5-small\"),\n",
    "    \"results\": os.path.abspath(f\"{project_root}/training/results\"),\n",
    "}\n",
    "\n",
    "# os.makedirs(paths[\"models\"], exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dataset(self, datanum: int = 1000) -> pd.DataFrame:\n",
    "    if os.path.exists(paths[\"qtrain\"]):\n",
    "        old_data = pd.read_csv(paths[\"qtrain\"])\n",
    "        new_questions = pd.concat([old_data, generate_text(datanum)])\n",
    "    else:\n",
    "        new_questions = generate_text(datanum)\n",
    "    \n",
    "    new_questions.to_csv(paths[\"qtrain\"], index=False)\n",
    "    return new_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_union_vocab(t5_tokenizer: T5Tokenizer, df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Xây dựng từ điển union giữa từ điển của T5 tokenizer và các token lấy từ cột 'question' của df.\n",
    "    Dữ liệu của generate_text được xử lý bằng cách tách theo khoảng trắng.\n",
    "    \"\"\"\n",
    "    tokens_set = set()\n",
    "    for text in df[\"question\"].tolist():\n",
    "        tokens = text.strip().split()\n",
    "        tokens_set.update(tokens)\n",
    "    # Lấy từ điển của T5 tokenizer\n",
    "    t5_vocab = set(t5_tokenizer.get_vocab().keys())\n",
    "    # Union của cả hai tập token\n",
    "    union_tokens = t5_vocab.union(tokens_set)\n",
    "    # Sắp xếp và tạo mapping token -> index\n",
    "    vocab = {token: idx for idx, token in enumerate(sorted(union_tokens))}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, corpus, window_size=5):\n",
    "        \"\"\"\n",
    "        corpus: list of list of token indices (sử dụng union_vocab)\n",
    "        window_size: kích thước cửa sổ context\n",
    "        \"\"\"\n",
    "        self.pairs = []\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "            for i, center in enumerate(sentence):\n",
    "                # Lấy các từ trong cửa sổ xung quanh (ngoại trừ từ trung tâm)\n",
    "                for j in range(max(0, i - window_size), min(sent_len, i + window_size + 1)):\n",
    "                    if i != j:\n",
    "                        self.pairs.append((center, sentence[j]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng cặp: 20\n",
      "Cặp 0: (2, 3)\n",
      "Cặp 1: (2, 0)\n",
      "Cặp 2: (3, 2)\n",
      "Cặp 3: (3, 0)\n",
      "Cặp 4: (0, 2)\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    [2, 3, 0],          # câu: \"this is hello\"\n",
    "    [0, 1, 2, 3, 4],    # câu: \"hello world this is test\"      \n",
    "]\n",
    "\n",
    "# Sử dụng window_size=2 để lấy các cặp\n",
    "dataset = Word2VecDataset(corpus, window_size=2)\n",
    "print(\"Số lượng cặp:\", len(dataset))\n",
    "for i in range(min(5, len(dataset))):\n",
    "    print(f\"Cặp {i}: {dataset[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        # embedding cho từ trung tâm và context\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, center, context, negative_context):\n",
    "        # center: (batch,)\n",
    "        # context: (batch,)\n",
    "        # negative_context: (batch, neg_samples)\n",
    "        center_embed = self.in_embeddings(center)           # (batch, embedding_dim)\n",
    "        context_embed = self.out_embeddings(context)         # (batch, embedding_dim)\n",
    "        # Tính điểm cho cặp (center, context) dương\n",
    "        score = (center_embed * context_embed).sum(dim=1)\n",
    "        log_target = torch.log(torch.sigmoid(score) + 1e-10)\n",
    "        \n",
    "        # Tính điểm cho các cặp âm (negative samples)\n",
    "        neg_embed = self.out_embeddings(negative_context)    # (batch, neg_samples, embedding_dim)\n",
    "        neg_score = torch.bmm(neg_embed, center_embed.unsqueeze(2)).squeeze()\n",
    "        log_negative = torch.log(torch.sigmoid(-neg_score) + 1e-10).sum(dim=1)\n",
    "        \n",
    "        loss = -(log_target + log_negative).mean()\n",
    "        return loss\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.in_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
