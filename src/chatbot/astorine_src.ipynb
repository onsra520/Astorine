{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\" if torch.cuda.is_available() else \"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Could not load en_core_web_trf or en_core_web_sm. Falling back to en_core_web_sm.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading spaCy models: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "nlp_pool = [nlp]\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        for _ in range(min(3, os.cpu_count() or 1)):\n",
    "            try:\n",
    "                nlp_pool.append(spacy.load(\"en_core_web_trf\"))\n",
    "            except OSError:\n",
    "                nlp_pool.append(spacy.load(\"en_core_web_sm\"))\n",
    "    else:\n",
    "        for _ in range(os.cpu_count() or 1):\n",
    "            nlp_pool.append(spacy.load(\"en_core_web_sm\"))\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "def process_documents(texts: List[str]) -> List:\n",
    "    return list(nlp.pipe(texts, batch_size=64))\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def lemmatize_cached(word: str) -> str:\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_.lower()\n",
    "\n",
    "COMMON_LEMMAS = {}\n",
    "def populate_common_lemmas():\n",
    "    \"\"\"Populate dictionary with common English words to avoid repeated computation.\"\"\"\n",
    "    common_words = [\n",
    "        \"the\", \"be\", \"to\", \"of\", \"and\", \"a\", \"in\", \"that\", \"have\", \"I\", \n",
    "        \"it\", \"for\", \"not\", \"on\", \"with\", \"he\", \"as\", \"you\", \"do\", \"at\",\n",
    "        \"this\", \"but\", \"his\", \"by\", \"from\", \"they\", \"we\", \"say\", \"her\", \"she\"\n",
    "        ]\n",
    "    for word in common_words:\n",
    "        COMMON_LEMMAS[word] = lemmatize_cached(word)\n",
    "populate_common_lemmas()\n",
    "\n",
    "def tokenize_parallel(sentences: List[str]) -> List[List[str]]:\n",
    "    def process_sentence(args):\n",
    "        sentence, nlp_instance = args\n",
    "        doc = nlp_instance(sentence)\n",
    "        return [token.text for token in doc]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(nlp_pool)) as executor:\n",
    "        work_items = [(sentence, nlp_pool[i % len(nlp_pool)]) for i, sentence in enumerate(sentences)]\n",
    "        results = list(executor.map(process_sentence, work_items))\n",
    "    return results\n",
    "\n",
    "def tokenize(sentence: str) -> list:\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def stem(word: str) -> str:\n",
    "    if word in COMMON_LEMMAS:\n",
    "        return COMMON_LEMMAS[word]\n",
    "    return lemmatize_cached(word)\n",
    "\n",
    "def batch_process_stems(words: List[str]) -> List[str]:\n",
    "    results = []\n",
    "    batch_words = []\n",
    "    batch_indices = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in COMMON_LEMMAS:\n",
    "            results.append(COMMON_LEMMAS[word])\n",
    "        else:\n",
    "            batch_words.append(word)\n",
    "            batch_indices.append(i)\n",
    "    if batch_words:\n",
    "        docs = list(nlp.pipe(batch_words, batch_size=64))\n",
    "        for i, doc in enumerate(docs):\n",
    "            lemma = doc[0].lemma_.lower()\n",
    "            original_idx = batch_indices[i]\n",
    "            results.insert(original_idx, lemma)\n",
    "            if batch_words[i] not in COMMON_LEMMAS:\n",
    "                COMMON_LEMMAS[batch_words[i]] = lemma\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_bow_matrix(tokenized_sentences: List[List[str]], all_words: List[str]) -> np.ndarray:\n",
    "    word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
    "    matrix_cpu = np.zeros((len(tokenized_sentences), len(all_words)), dtype=np.float32)\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        lemmas = set(batch_process_stems(sentence))\n",
    "        for lemma in lemmas:\n",
    "            if lemma in word_to_idx:\n",
    "                matrix_cpu[i, word_to_idx[lemma]] = 1.0\n",
    "    return matrix_cpu\n",
    "\n",
    "def bag_of_words(tokenized_sentence: list, all_words: list) -> np.ndarray:\n",
    "    lemmas = set(batch_process_stems(tokenized_sentence))\n",
    "    word_to_idx = getattr(bag_of_words, \"word_to_idx\", None)\n",
    "    if word_to_idx is None or len(word_to_idx) != len(all_words):\n",
    "        bag_of_words.word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
    "        word_to_idx = bag_of_words.word_to_idx\n",
    "    bag_cpu = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for lemma in lemmas:\n",
    "        if lemma in word_to_idx:\n",
    "            bag_cpu[word_to_idx[lemma]] = 1.0\n",
    "    \n",
    "    return bag_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.3):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size * 2) \n",
    "        self.ln2 = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.l3 = nn.Linear(hidden_size * 2, hidden_size) \n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.l4 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        identity = out\n",
    "        out = self.l2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.relu(out)\n",
    "        if hasattr(self, 'input_size') and self.input_size == self.hidden_size:\n",
    "            out = out + identity \n",
    "        out = self.dropout3(out)\n",
    "\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "\n",
    "    def optimize(self, pruning_amount=0.3):\n",
    "        for _, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear) and hasattr(module, 'weight_orig'):\n",
    "                prune.remove(module, 'weight')\n",
    "        for _, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
    "        half_model = self.half()\n",
    "        try:\n",
    "            scripted_model = torch.jit.script(half_model)\n",
    "            return scripted_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scripting: {e}\")\n",
    "            return half_model\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__init__(state['input_size'], state['hidden_size'], state['num_classes'], state['dropout_rate'])\n",
    "        self.load_state_dict(state['state_dict'])\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = torch.FloatTensor(np.array(X_train))\n",
    "        self.y_data = torch.LongTensor(np.array(Y_train))\n",
    "\n",
    "            \n",
    "        if isinstance(Y_train, np.ndarray):\n",
    "            self.y_data = torch.LongTensor(Y_train)\n",
    "        else:\n",
    "            self.y_data = torch.LongTensor(np.array(Y_train))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbottraining:\n",
    "    def __init__(\n",
    "        self,         \n",
    "        path: str,\n",
    "        training_data: dict,      \n",
    "        hidden_size: int = 64,\n",
    "        batch_size: int = 32,\n",
    "        dropout_rate: float = 0.2,\n",
    "        learning_rate: float = 3e-4,\n",
    "        num_epochs: int = 200,\n",
    "        validation_split: float = 0.2, \n",
    "        patience: int = 10,\n",
    "        use_cuda: bool = True\n",
    "    ) -> None:\n",
    "        \n",
    "        self.intents_data = training_data\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs    \n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        X_train, Y_train = self._prepare_dataset()\n",
    "        self.input_size = len(X_train[0])\n",
    "        self.output_size = len(self.tags)\n",
    "        dataset_size = len(X_train)\n",
    "        indices = list(range(dataset_size))\n",
    "        val_split = int(np.floor(self.validation_split * dataset_size))\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_indices, val_indices = indices[val_split:], indices[:val_split]\n",
    "        X_train_split = [X_train[i] for i in train_indices]\n",
    "        Y_train_split = [Y_train[i] for i in train_indices]\n",
    "        X_val = [X_train[i] for i in val_indices]\n",
    "        Y_val = [Y_train[i] for i in val_indices]\n",
    "        \n",
    "        train_dataset = DialogueDataset(X_train_split, Y_train_split)\n",
    "        val_dataset = DialogueDataset(X_val, Y_val)\n",
    "        num_workers = 0 \n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=self.use_cuda\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            dataset=val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=self.use_cuda\n",
    "        )\n",
    "        \n",
    "        self._training(path)\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        self.all_words, self.tags, xy = [], [], []\n",
    "        ignore_words = ['?', '!', '.', ',']\n",
    "        \n",
    "        patterns_list = []\n",
    "        tags_list = []\n",
    "\n",
    "        total_steps = 100\n",
    "        with tqdm(total=total_steps, desc=\"Preparing the training dataset\") as main_bar:\n",
    "            with tqdm(total=len(self.intents_data['intents']), desc=\"Tokenizing patterns in parallel\", leave=False) as sub_bar1:\n",
    "                for intent in self.intents_data['intents']:\n",
    "                    if \"tag\" in intent and \"patterns\" in intent:\n",
    "                        tag = intent['tag']\n",
    "                        self.tags.append(tag)\n",
    "                        for pattern in intent['patterns']:\n",
    "                            patterns_list.append(pattern.lower())\n",
    "                            tags_list.append(tag)\n",
    "                        \n",
    "                        all_tokenized_patterns = tokenize_parallel(patterns_list)\n",
    "                        for i, tokenized_pattern in enumerate(all_tokenized_patterns):\n",
    "                            self.all_words.extend(tokenized_pattern)\n",
    "                            xy.append((tokenized_pattern, tags_list[i]))\n",
    "                        sub_bar1.update(1)\n",
    "                main_bar.update(25) \n",
    "                    \n",
    "            with tqdm(total=100, desc=\"Processing word stems\", leave=False) as sub_bar2:           \n",
    "                filtered_words = [word for word in self.all_words if word not in ignore_words]\n",
    "                sub_bar2.update(25)\n",
    "                \n",
    "                stemmed_words = batch_process_stems(filtered_words)\n",
    "                sub_bar2.update(50)\n",
    "                \n",
    "                self.all_words = sorted(set(stemmed_words))\n",
    "                sub_bar2.update(25)\n",
    "                main_bar.update(25)\n",
    "\n",
    "            with tqdm(total=len(xy), desc=\"Augmenting dataset\", leave=False) as sub_bar3:\n",
    "                augmented_xy = []\n",
    "                for pattern_words, tag in xy:\n",
    "                    augmented_xy.append((pattern_words, tag))\n",
    "                    if len(pattern_words) > 3: \n",
    "                        dropout_words = pattern_words.copy()\n",
    "                        drop_idx = np.random.randint(0, len(pattern_words))\n",
    "                        dropout_words.pop(drop_idx)\n",
    "                        augmented_xy.append((dropout_words, tag))\n",
    "                    if len(pattern_words) > 3: \n",
    "                        shuffled_words = pattern_words.copy()\n",
    "                        if len(shuffled_words) > 1:\n",
    "                            i, j = np.random.choice(range(len(shuffled_words)), 2, replace=False)\n",
    "                            shuffled_words[i], shuffled_words[j] = shuffled_words[j], shuffled_words[i]\n",
    "                        augmented_xy.append((shuffled_words, tag))\n",
    "                    sub_bar3.update(1)\n",
    "                main_bar.update(25)\n",
    "                    \n",
    "            with tqdm(total=len(xy), desc=\"Creating bag-of-words vectors\", leave=False) as sub_bar4:\n",
    "                xy = augmented_xy\n",
    "                X_train = []\n",
    "                Y_train = []\n",
    "                batch_size = 100 \n",
    "                for i in range(0, len(xy), batch_size):\n",
    "                    batch_xy = xy[i:i+batch_size]\n",
    "                    batch_sentences = [pattern for pattern, _ in batch_xy]\n",
    "                    batch_tags = [tag for _, tag in batch_xy]\n",
    "                    batch_bow = create_bow_matrix(batch_sentences, self.all_words)\n",
    "                    for j, tag in enumerate(batch_tags):\n",
    "                        X_train.append(batch_bow[j])\n",
    "                        label = self.tags.index(tag)\n",
    "                        Y_train.append(label)\n",
    "                    sub_bar4.update(len(batch_xy))\n",
    "                main_bar.update(25)\n",
    "        return X_train, Y_train\n",
    "\n",
    "    def _training(self, path: str) -> None:\n",
    "        device = torch.device('cuda' if self.use_cuda else 'cpu')\n",
    "        model = CustomNN(\n",
    "            self.input_size, \n",
    "            self.hidden_size, \n",
    "            self.output_size, \n",
    "            self.dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=1e-4  \n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=5\n",
    "        )\n",
    "        scaler = torch.amp.GradScaler('cuda') if self.use_cuda else None\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        device_info = {}\n",
    "        if self.use_cuda:\n",
    "            current_device = torch.cuda.current_device()\n",
    "            device_info['Device'] = f\"GPU: {torch.cuda.get_device_name(current_device)}\"\n",
    "            device_info['VRAM'] = f\"{torch.cuda.get_device_properties(current_device).total_memory / 1e9:.2f} GB\"\n",
    "        else:\n",
    "            device_info['Device'] = \"CPU\"\n",
    "\n",
    "        with tqdm(total=self.num_epochs, desc=\"Training\") as epoch_bar:\n",
    "            try:\n",
    "                for epoch in range(self.num_epochs):\n",
    "                    model.train()\n",
    "                    epoch_loss = 0.0\n",
    "                    batch_count = 0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    train_loader_iter = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs} [Train]\", leave=False)\n",
    "                    \n",
    "                    for (words, labels) in train_loader_iter:\n",
    "                        words = words.to(device, dtype=torch.float)\n",
    "                        labels = labels.to(device, dtype=torch.long)\n",
    "                        \n",
    "                        if self.use_cuda:\n",
    "                            with torch.amp.autocast('cuda'):\n",
    "                                outputs = model(words)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                        \n",
    "                            optimizer.zero_grad()\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                            \n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            outputs = model(words)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                            optimizer.step()\n",
    "                        epoch_loss += loss.item()\n",
    "                        batch_count += 1\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        train_acc = correct / total if total > 0 else 0\n",
    "                        train_loader_iter.set_postfix({\n",
    "                            'loss': f\"{loss.item():.4f}\",\n",
    "                            'acc': f\"{train_acc:.4f}\"\n",
    "                        })\n",
    "                    \n",
    "                    train_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "                    train_acc = correct / total if total > 0 else 0\n",
    "                    model.eval()\n",
    "                    val_loss = 0.0\n",
    "                    val_batch_count = 0\n",
    "                    val_correct = 0\n",
    "                    val_total = 0\n",
    "                    \n",
    "                    val_loader_iter = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs} [Val]\", leave=False)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for (words, labels) in val_loader_iter:\n",
    "                            words = words.to(device, dtype=torch.float) \n",
    "                            labels = labels.to(device, dtype=torch.long)\n",
    "                            \n",
    "                            if self.use_cuda:\n",
    "                                with torch.amp.autocast('cuda'):\n",
    "                                    outputs = model(words)\n",
    "                                    loss = criterion(outputs, labels)\n",
    "                            else:\n",
    "                                outputs = model(words)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            val_loss += loss.item()\n",
    "                            val_batch_count += 1\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            val_total += labels.size(0)\n",
    "                            val_correct += (predicted == labels).sum().item()\n",
    "                            val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "                            val_loader_iter.set_postfix({\n",
    "                                'loss': f\"{loss.item():.4f}\",\n",
    "                                'acc': f\"{val_acc:.4f}\"\n",
    "                            })\n",
    "                    \n",
    "                    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else 0\n",
    "                    val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "                    scheduler.step(avg_val_loss)\n",
    "\n",
    "                    epoch_bar.update(1)\n",
    "                    epoch_bar.set_description(f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "                    epoch_bar.set_postfix({\n",
    "                        **device_info,\n",
    "                        'Train Loss': f\"{train_loss:.4f}\",\n",
    "                        'Train Acc': f\"{train_acc:.4f}\",\n",
    "                        'Val Loss': f\"{avg_val_loss:.4f}\",\n",
    "                        'Val Acc': f\"{val_acc:.4f}\"\n",
    "                    })\n",
    "\n",
    "                    if early_stop_counter >= self.patience:                              \n",
    "                        print(f\"Early stopping at epoch {epoch+1} with validation loss {best_val_loss:.4f}\")\n",
    "                        break\n",
    "\n",
    "                    if train_loss < 0.01 and avg_val_loss < 0.1:\n",
    "                        print(f\"Reached desired performance at epoch {epoch+1}\")\n",
    "                        break\n",
    "                    \n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        early_stop_counter = 0\n",
    "                        best_model_state = model.state_dict().copy()\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                \n",
    "                if best_model_state:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    \n",
    "                print(f'Final validation loss: {best_val_loss:.4f}')\n",
    "                checkpoint = {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"input_size\": self.input_size,\n",
    "                    \"hidden_size\": self.hidden_size,\n",
    "                    \"output_size\": self.output_size,\n",
    "                    \"all_words\": self.all_words,\n",
    "                    \"tags\": self.tags\n",
    "                }\n",
    "                torch.save(checkpoint, path)\n",
    "                print(f\"Model saved to {path}\")\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError occurred: {str(e)}\")\n",
    "                print(f\"Current device: {device}\")\n",
    "                if self.use_cuda:\n",
    "                    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "                    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "                if self.use_cuda:\n",
    "                    print(\"Attempting to fall back to CPU...\")\n",
    "                    self.use_cuda = False\n",
    "                    self._training(path)\n",
    "                else:\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot():\n",
    "    hidden_size: int = 64 \n",
    "    batch_size: int = 32 \n",
    "    dropout_rate: float = 0.2 \n",
    "    learning_rate: float = 3e-4  \n",
    "    num_epochs: int = 200      \n",
    "    validation_split: float = 0.2\n",
    "    patience: int = 10\n",
    "    use_cuda: bool = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        training_data: dict, \n",
    "        train: bool = False, \n",
    "        fine_tune: bool = False,\n",
    "        hidden_size: int = None,\n",
    "        batch_size: int = None,\n",
    "        dropout_rate: float = None,\n",
    "        learning_rate: float = None,\n",
    "        num_epochs: int = None,\n",
    "        validation_split: float = None,\n",
    "        patience: int = None,\n",
    "        use_cuda: bool = True\n",
    "        ) -> None:\n",
    "        \n",
    "        self.hidden_size = hidden_size if hidden_size is not None else chatbot.hidden_size\n",
    "        self.batch_size = batch_size if batch_size is not None else chatbot.batch_size\n",
    "        self.dropout_rate = dropout_rate if dropout_rate is not None else chatbot.dropout_rate\n",
    "        self.learning_rate = learning_rate if learning_rate is not None else chatbot.learning_rate\n",
    "        self.num_epochs = num_epochs if num_epochs is not None else chatbot.num_epochs\n",
    "        self.validation_split = validation_split if validation_split is not None else chatbot.validation_split\n",
    "        self.patience = patience if patience is not None else chatbot.patience\n",
    "        self.use_cuda = use_cuda if use_cuda is not None else chatbot.use_cuda\n",
    "        \n",
    "        if train == True and training_data and path:\n",
    "            _ = chatbottraining(\n",
    "                path=path,\n",
    "                training_data=training_data,\n",
    "                hidden_size=self.hidden_size,\n",
    "                batch_size=self.batch_size,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                learning_rate=self.learning_rate,\n",
    "                num_epochs=self.num_epochs,\n",
    "                validation_split=0.2,\n",
    "                patience=15 \n",
    "            )    \n",
    "        \n",
    "        if fine_tune and training_data and path:\n",
    "            self._fine_tune_model(path, training_data)\n",
    "            \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.intents_data = training_data\n",
    "        self.responses_dict = {\n",
    "            intent[\"tag\"]: intent[\"responses\"]\n",
    "            for intent in self.intents_data[\"intents\"]\n",
    "            if \"tag\" in intent and \"responses\" in intent\n",
    "        }\n",
    "        self._load_model(path = path)\n",
    "\n",
    "    def _fine_tune_model(self, path, training_data):\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        _ = chatbottraining(\n",
    "            path=path,\n",
    "            training_data=training_data,\n",
    "            hidden_size=checkpoint[\"hidden_size\"],\n",
    "            batch_size=self.batch_size,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            learning_rate=self.learning_rate / 10, \n",
    "            num_epochs=self.num_epochs // 2,\n",
    "            validation_split=self.validation_split,\n",
    "            patience=self.patience,\n",
    "            use_cuda=self.use_cuda\n",
    "        )\n",
    "\n",
    "    def _load_model(self, path: str) -> None:\n",
    "        \"\"\"Load the trained model with error handling.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.model = CustomNN(\n",
    "                checkpoint[\"input_size\"], \n",
    "                checkpoint[\"hidden_size\"], \n",
    "                checkpoint[\"output_size\"],\n",
    "                dropout_rate=self.dropout_rate\n",
    "            ).to(self.device)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state\"])\n",
    "            self.model.eval()\n",
    "            self.all_words = checkpoint[\"all_words\"]\n",
    "            self.tags = checkpoint[\"tags\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load model from {path}: {str(e)}\")\n",
    "            \n",
    "    def predict_intent(self, query: str = None) -> str:\n",
    "        if not query:\n",
    "            return \"unknown\"\n",
    "            \n",
    "        words = tokenize(query)\n",
    "        stemmed_words = [stem(word) for word in words]\n",
    "        bag = bag_of_words(stemmed_words, self.all_words)\n",
    "        bag = torch.from_numpy(bag).float().to(self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(bag)\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            \n",
    "        tag_idx = predicted.item()\n",
    "        prob = probs[0][tag_idx].item()\n",
    "        top_probs, _ = torch.topk(probs, 2, dim=1)\n",
    "        top_prob_diff = top_probs[0][0].item() - top_probs[0][1].item()\n",
    "        confidence_threshold = 0.6 if top_prob_diff > 0.3 else 0.75\n",
    "        \n",
    "        if prob > confidence_threshold:\n",
    "            return self.tags[tag_idx]\n",
    "        return \"unknown\"\n",
    "\n",
    "    def get_response(self, tag: str):\n",
    "        \"\"\"Get response for the predicted intent.\"\"\"\n",
    "        if tag in self.responses_dict:\n",
    "            responses = self.responses_dict[tag]\n",
    "            # Select response with weighting toward more specific responses\n",
    "            if len(responses) > 1:\n",
    "                # Calculate response length as a rough approximation of specificity\n",
    "                response_weights = [len(r) for r in responses]\n",
    "                total_weight = sum(response_weights)\n",
    "                probabilities = [w/total_weight for w in response_weights]\n",
    "                return np.random.choice(responses, p=probabilities)\n",
    "            return random.choice(responses)\n",
    "        else:\n",
    "            return \"I'm sorry, I don't understand that.\"\n",
    "        \n",
    "def reply(query: str, path: str, training_data: dict) -> str:        \n",
    "    chatbot_name = \"Astorine\"\n",
    "    chat = chatbot(\n",
    "        path = path,\n",
    "        training_data = training_data\n",
    "    )\n",
    "    intent = chat.predict_intent(query)\n",
    "    response = chat.get_response(intent)\n",
    "    return f\"{chatbot_name}: {response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from astorine import reply, chatbot\n",
    "from nlp.helper.ibuilder import igenerate_lite\n",
    "from nlp.extractor import extract\n",
    "from handlers.rcm import searching\n",
    "from IPython.display import clear_output  \n",
    "from decimal import Decimal\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\\\\chatbotmodel.pth\" ) \n",
    "intents_dir = os.path.join(os.getcwd(), \"intents\\\\intents_lite.json\" )\n",
    "data =  igenerate_lite(save=True, save_dir=intents_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = {}\n",
    "\n",
    "required_fields = [\n",
    "    \"brand\", \"gpu\", \"cpu\", \"ram\", \"resolution\", \"refresh rate\", \n",
    "    \"display type\", \"screen size\", \"use_for\", \"price\"\n",
    "]\n",
    "\n",
    "class ChatbotSession:\n",
    "    def __init__(self):\n",
    "        self.current_flow = \"Nothing\"  # \"guided\", \"search\", hoặc \"faq\"\n",
    "        self.context = None            # Ví dụ: \"price\" khi hỏi về giá\n",
    "        self.criteria = {              # Lưu trữ tiêu chí\n",
    "            \"brand\": None,\n",
    "            \"gpu\": None,\n",
    "            \"cpu\": None,\n",
    "            \"ram\": None,\n",
    "            \"resolution\": None,\n",
    "            \"refresh rate\": None,\n",
    "            \"display type\": None,\n",
    "            \"screen size\": None,\n",
    "            \"use_for\": None,\n",
    "            \"price\": {'min': Decimal('0'), 'max': Decimal('0')}\n",
    "        }\n",
    "        self.previous_flow = None   \n",
    "\n",
    "def get_session(user_id):\n",
    "    if user_id not in sessions:\n",
    "        sessions[user_id] = ChatbotSession()\n",
    "    return sessions[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_session(session: ChatbotSession, user_input: str):\n",
    "    extracted = extract(user_input)\n",
    "    for key, value in extracted.items():\n",
    "        if value is not None and (key != \"price\" or session.context == \"price\" or session.current_flow == \"search\"):\n",
    "            session.criteria[key] = value\n",
    "\n",
    "    tag_response = reply(user_input)\n",
    "    tag = tag_response.get(\"tag\")\n",
    "    response = tag_response.get(\"response\")\n",
    "\n",
    "    if tag == \"help\":\n",
    "        session.current_flow = \"guided\"\n",
    "        session.context = None\n",
    "        return response\n",
    "\n",
    "    collected_criteria = sum(\n",
    "        1\n",
    "        for key, value in session.criteria.items()\n",
    "        if key not in [\"price\", \"use_for\"] and value is not None and value != \"\"\n",
    "    )\n",
    "\n",
    "    if collected_criteria >= 3:\n",
    "        session.current_flow = \"search\"\n",
    "        if (session.criteria.get(\"price\") == {'min': Decimal('0'), 'max': Decimal('0')} or tag == \"use_for\"):\n",
    "            session.context = \"price\"\n",
    "            return \"Please specify your price range for the laptop.\"\n",
    "        else:\n",
    "            criteria_string = \" | \".join(str(session.criteria.get(field) or \"\") for field in required_fields)\n",
    "            results = searching(criteria_string)\n",
    "            if results:\n",
    "                formatted_results = \"\\n\".join([f\"{i+1}. {laptop}\" for i, laptop in enumerate(results)])\n",
    "                return f\"Here are the laptops I found:\\n{formatted_results}\\nI think they are good for you.\"\n",
    "            else:\n",
    "                return \"I couldn't find any laptops matching your criteria.\"\n",
    "\n",
    "    if session.context == \"price\":\n",
    "        if extracted.get(\"price\") is not None and extracted[\"price\"] != {'min': Decimal('0'), 'max': Decimal('0')}:\n",
    "            session.criteria[\"price\"] = extracted[\"price\"]\n",
    "            criteria_string = \" | \".join(str(session.criteria.get(field) or \"\") for field in required_fields)\n",
    "            results = searching(criteria_string)\n",
    "            session.current_flow = \"search\"\n",
    "            if results:\n",
    "                formatted_results = \"\\n\".join([f\"{i+1}. {laptop}\" for i, laptop in enumerate(results)])\n",
    "                return f\"Here are the laptops I found:\\n{formatted_results}\\nI think they are good for you.\"\n",
    "            else:\n",
    "                return \"I couldn't find any laptops matching your criteria.\"\n",
    "        else:\n",
    "            return \"Please provide your desired price range.\"\n",
    "\n",
    "    faq_tags = [\"gpu_question\", \"cpu_question\", \"ram_question\"]\n",
    "    if session.current_flow == \"guided\" and tag in faq_tags:\n",
    "        faq_response = response\n",
    "        resume_message = f\"{faq_response} So, what {tag.split('_')[0]} do you prefer?\"\n",
    "        return resume_message\n",
    "\n",
    "    if tag == \"use_for\":\n",
    "        session.criteria[\"use_for\"] = extracted.get(\"use_for\", user_input)\n",
    "        session.context = \"price\"\n",
    "        return \"Please specify your price range for the laptop.\"\n",
    "\n",
    "    if session.current_flow == \"guided\":\n",
    "        for field in required_fields:\n",
    "            if field not in session.criteria or session.criteria[field] is None or session.criteria[field] == \"\":\n",
    "                if field == \"use_for\":\n",
    "                    return \"What will you use the laptop for?\"\n",
    "                elif field == \"price\":\n",
    "                    return \"Please specify your price range for the laptop.\"\n",
    "                else:\n",
    "                    return f\"Please tell me your preferred {field}.\"\n",
    "\n",
    "        criteria_string = \" | \".join(str(session.criteria.get(field) or \"\") for field in required_fields)\n",
    "        results = searching(criteria_string)\n",
    "        session.current_flow = \"search\"\n",
    "        if results:\n",
    "            formatted_results = \"\\n\".join([f\"{i+1}. {laptop}\" for i, laptop in enumerate(results)])\n",
    "            return f\"Here are the laptops I found:\\n{formatted_results}\\nI think they are good for you.\"\n",
    "        else:\n",
    "            return \"I couldn't find any laptops matching your criteria.\"\n",
    "\n",
    "    return response\n",
    "\n",
    "def chatbot_handle(user_id: str, user_input: str):\n",
    "    session = get_session(user_id)\n",
    "    response = update_session(session, user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"userdeptraisiucapvutru\"\n",
    "\n",
    "print(chatbot_handle(user_id, \"recommend me a laptop have rtx 4060, intel core i9 12th, 32gb ram, 17 inch display\"))\n",
    "print(chatbot_handle(user_id, \"price range is $1000 to $2000\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astorine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
