{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.append(str(Path(__file__).resolve().parents[1]))\n",
    "\n",
    "from utils.nlp_utils import tokenize, stem, bag_of_words\n",
    "from utils.ncomp import rlst, srlst, clst, glst, rrlst, dtlst, sslst, blst\n",
    "from handlers.rcm import searching\n",
    "# from nlp.ChatModel import NeuralNetwork, ChatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(__file__).resolve().parents[1]\n",
    "\n",
    "paths = {\n",
    "    \"intents\": os.path.abspath(f\"{project_root}/intents\"),\n",
    "    \"patterns\": os.path.abspath(f\"{project_root}/intents/patterns.json\"),\n",
    "    \"responses\": os.path.abspath(f\"{project_root}/intents/responses.json\"),\n",
    "    \"replymodel\": os.path.abspath(f\"{project_root}/models/replymodel.pth\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "intent_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.5783\n",
      "Epoch [200/500], Loss: 0.6499\n",
      "Epoch [300/500], Loss: 0.5844\n",
      "Epoch [400/500], Loss: 1.2764\n",
      "Epoch [500/500], Loss: 0.7132\n",
      "final loss: 0.7132\n",
      "training complete. file saved to C:\\Users\\trtie\\OneDrive - camann\\Documents\\GitHub - Repository\\Astorine\\src\\chatbot\\models\\replymodel.pth\n"
     ]
    }
   ],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.3):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "    def optimize(self, pruning_amount=0.3):\n",
    "        for _, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
    "        self.half()\n",
    "        scripted_model = torch.jit.script(self)\n",
    "        return scripted_model\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__init__(state['input_size'], state['hidden_size'], state['num_classes'], state['dropout_rate'])\n",
    "        self.load_state_dict(state['state_dict'])\n",
    "        \n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = Y_train\n",
    "\n",
    "    # Hỗ trợ việc truy cập bằng chỉ số để có thể lấy mẫu thứ i trong tập dữ liệu bằng cách dùng dataset[i].\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # Chúng ta có thể gọi len(dataset) để trả về kích thước.\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "with open(paths[\"patterns\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    intents = json5.load(f)\n",
    "\n",
    "all_words, tags, xy = [], [], []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        word = tokenize(pattern)\n",
    "        all_words.extend(word)\n",
    "        xy.append((word, tag))\n",
    "        \n",
    "# Xóa các từ trùng lặp và sắp xếp chúng\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "# Tạo dữ liệu huấn luyện\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: Túi từ cho mỗi câu mẫu (pattern_sentence).\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # Y: PyTorch CrossEntropyLoss chỉ cần class labels, không cần one-hot.\n",
    "    label = tags.index(tag)\n",
    "    Y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "# Hyper-parameters - Các siêu tham số\n",
    "num_epochs = 500 # Số lần lặp qua toàn bộ dữ liệu huấn luyện\n",
    "batch_size = 8 # Số mẫu dữ liệu được xử lý cùng một lúc trong mỗi lần cập nhật trọng số.\n",
    "learning_rate = 0.001 # Tốc độ học, điều chỉnh độ lớn của bước cập nhật trọng số.\n",
    "input_size = len(X_train[0]) # Kích thước đầu vào của mô hình, bằng số lượng từ trong bộ từ vựng (all_words).\n",
    "hidden_size = 8 # Kích thước của lớp ẩn (hidden layer) trong mô hình.\n",
    "output_size = len(tags) # Kích thước đầu ra của mô hình, bằng số lượng nhãn (tags).\n",
    "\n",
    "dataset = DialogueDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Nếu có GPU thì sử dụng, nếu không thì sử dụng CPU\n",
    "model = CustomNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Huấn luyện mô hình qua nhiều epoch.\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass - Lan truyền tiến là quá trình mô hình tính toán đầu ra (output) dựa trên dữ liệu đầu vào (input)\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels) # Tính toán loss giữa outputs và labels.\n",
    "\n",
    "        # Backward and optimize - Lan truyền ngược và tối ưu hóa\n",
    "        optimizer.zero_grad() # Xóa gradient từ trước đó, tránh việc gradient tích lũy.\n",
    "        loss.backward() # thực hiện lan truyền ngược, tính toán gradient của loss đối với tất cả các tham số của mô hình bằng cách sử dụng quy tắc chuỗi (chain rule).\n",
    "        optimizer.step() # cập nhật trọng số của mô hình dựa trên gradient đã tính toán.\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "torch.save(data, paths[\"replymodel\"]) \n",
    "print(f'training complete. file saved to {paths[\"replymodel\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: halo\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(paths[\"replymodel\"])\n",
    "model = CustomNN(data[\"input_size\"], data[\"hidden_size\"], data[\"output_size\"]).to(device)\n",
    "model.load_state_dict(data[\"model_state\"])\n",
    "model.eval()\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "def predict_intent(user_input):\n",
    "    words = tokenize(user_input)\n",
    "    bag = bag_of_words(words, all_words)\n",
    "    bag = torch.from_numpy(bag).float().to(device)\n",
    "    bag = bag.unsqueeze(0)  # Thêm batch dimension: [input_size] -> [1, input_size]\n",
    "    with torch.no_grad():\n",
    "        output = model(bag)  # output sẽ có kích thước [1, num_classes]\n",
    "        _, predicted = torch.max(output, dim=1)  # predicted sẽ là [1]\n",
    "        tag = tags[predicted.item()]\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        if prob.item() > 0.75:  # Ngưỡng tin cậy\n",
    "            return tag\n",
    "        return \"unknown\"\n",
    "\n",
    "sessions = {}\n",
    "\n",
    "class ChatbotSession:\n",
    "    def __init__(self):\n",
    "        self.current_flow = \"guided\"  # 'guided' hoặc 'search'\n",
    "        self.context = None           # ví dụ: \"Need_help\", \"collecting_criteria\", v.v.\n",
    "        self.criteria = {}            # lưu trữ các tiêu chí: brand, gpu, cpu, ...\n",
    "        self.previous_flow = None     # lưu flow tạm thời khi trả lời FAQ\n",
    "\n",
    "def get_session(user_id):\n",
    "    if user_id not in sessions:\n",
    "        sessions[user_id] = ChatbotSession()\n",
    "    return sessions[user_id]\n",
    "\n",
    "def is_faq_question(text: str) -> bool:\n",
    "    faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "    return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "\n",
    "def is_direct_search_query(text: str) -> bool:\n",
    "    # Nếu câu có nhiều từ và chứa các từ khóa chỉ ra yêu cầu tìm kiếm laptop\n",
    "    return len(text.split()) > 15 and any(term in text.lower() for term in [\"suggest\", \"recommend\", \"find\", \"laptop\"])\n",
    "\n",
    "def is_faq_question(text: str) -> bool:\n",
    "    faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "    return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "\n",
    "def get_faq_answer(query: str) -> str:\n",
    "    faq_dict = {\n",
    "        \"what is gpu\": \"GPU (Graphics Processing Unit).\",\n",
    "        \"what is cpu\": \"CPU (Central Processing Unit).\",\n",
    "        \"what is ram\": \"RAM (Random Access Memory).\"\n",
    "    }\n",
    "    for key in faq_dict:\n",
    "        if key in query.lower():\n",
    "            return faq_dict[key]\n",
    "    return \"I don't have the answer to that question.\"\n",
    "\n",
    "def update_session_criteria(session: ChatbotSession, user_input: str):\n",
    "    lower_input = user_input.lower()\n",
    "    if \"brand\" in lower_input or any(brand in lower_input for brand in blst()):\n",
    "        session.criteria[\"brand\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"brand\"] = None\n",
    "    if \"gpu\" in lower_input or any(x in lower_input for x in glst()):\n",
    "        session.criteria[\"gpu\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"gpu\"] = None\n",
    "    if \"cpu\" in lower_input or any(x in lower_input for x in clst()):\n",
    "        session.criteria[\"cpu\"] = user_input\n",
    "    if \"ram\" in lower_input or any(x in lower_input for x in rlst()):\n",
    "        session.criteria[\"ram\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"ram\"] = None\n",
    "    if \"resolution\" in lower_input or any(x in lower_input for x in srlst()):\n",
    "        session.criteria[\"resolution\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"resolution\"] = None\n",
    "    if \"refresh\" in lower_input or any(x in lower_input for x in rrlst()):\n",
    "        session.criteria[\"refresh rate\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"refresh rate\"] = None\n",
    "    if \"display\" in lower_input or any(x in lower_input for x in dtlst()):\n",
    "        session.criteria[\"display type\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"display type\"] = None\n",
    "    if \"screen\" in lower_input or any(x in lower_input for x in sslst()):\n",
    "        session.criteria[\"screen size\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"screen size\"] = None\n",
    "\n",
    "def get_next_question(session: ChatbotSession) -> str:\n",
    "    questions = {\n",
    "        \"brand\": \"Of course! Let's start with the brand. Any preferences?\",\n",
    "        \"gpu\": \"Noted! Which GPU are you aiming for?\",\n",
    "        \"cpu\": \"Cool! Which processor do you prefer?\",\n",
    "        \"ram\": \"Alright! What's your RAM requirement?\",\n",
    "        \"resolution\": \"What screen resolution do you want?\",\n",
    "        \"refresh rate\": \"What is the desired screen refresh rate?\",\n",
    "        \"display type\": \"What type of screen do you prefer (IPS, OLED, ...)?\",\n",
    "        \"screen size\": \"Great choice! What screen size are you looking for?\"\n",
    "    }\n",
    "    for key in questions:\n",
    "        if key not in session.criteria:\n",
    "            return questions[key]\n",
    "    return \"🔍 Searching for the best options...\"\n",
    "\n",
    "def process_user_input(user_id: str, user_input: str):\n",
    "    session = get_session(user_id)\n",
    "    intent = predict_intent(user_input)\n",
    "\n",
    "    # Xử lý các ý định cụ thể\n",
    "    if intent == \"search\" or is_direct_search_query(user_input):\n",
    "        session.current_flow = \"search\"\n",
    "        return searching(user_input)\n",
    "    \n",
    "    elif intent == \"faq\" or is_faq_question(user_input):\n",
    "        prev_flow = session.current_flow\n",
    "        faq_answer = get_faq_answer(user_input)\n",
    "        session.current_flow = prev_flow\n",
    "        return faq_answer\n",
    "    \n",
    "    elif intent == \"start_guided\":\n",
    "        session.current_flow = \"guided\"\n",
    "        session.context = \"collecting_brand\"\n",
    "        return \"Let's start with the brand. Any preferences?\"\n",
    "    \n",
    "    elif session.current_flow == \"guided\":\n",
    "        update_session_criteria(session, user_input)\n",
    "        next_q = get_next_question(session)\n",
    "        if next_q == \"🔍 Searching for the best options...\":\n",
    "            return searching(session.criteria)\n",
    "        return next_q\n",
    "    \n",
    "    return \"I'm sorry, I don't understand that question.\"\n",
    "\n",
    "# Sau khi huấn luyện và lưu mô hình\n",
    "data = torch.load(paths[\"replymodel\"])\n",
    "model = CustomNN(data[\"input_size\"], data[\"hidden_size\"], data[\"output_size\"]).to(device)\n",
    "model.load_state_dict(data[\"model_state\"])\n",
    "model.eval()\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "user_id = \"user_123\"\n",
    "print(\"Chatbot: halo\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: thanks for chatting!\")\n",
    "        break\n",
    "    response = process_user_input(user_id, user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_direct_search_query(text: str) -> bool:\n",
    "#     # Nếu câu có nhiều từ và chứa các từ khóa chỉ ra yêu cầu tìm kiếm laptop\n",
    "#     return len(text.split()) > 15 and any(term in text.lower() for term in [\"suggest\", \"recommend\", \"find\", \"laptop\"])\n",
    "\n",
    "# def is_faq_question(text: str) -> bool:\n",
    "#     faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "#     return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "# def get_faq_answer(query: str) -> str:\n",
    "#     faq_dict = {\n",
    "#         \"what is gpu\": \"GPU (Graphics Processing Unit).\",\n",
    "#         \"what is cpu\": \"CPU (Central Processing Unit).\",\n",
    "#         \"what is ram\": \"RAM (Random Access Memory).\"\n",
    "#     }\n",
    "#     for key in faq_dict:\n",
    "#         if key in query.lower():\n",
    "#             return faq_dict[key]\n",
    "#     return \"I don't have the answer to that question.\"\n",
    "\n",
    "# def update_session_criteria(session: ChatbotSession, user_input: str):\n",
    "#     lower_input = user_input.lower()\n",
    "#     if \"brand\" in lower_input or any(brand in lower_input for brand in blst()):\n",
    "#         session.criteria[\"brand\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"brand\"] = None\n",
    "#     if \"gpu\" in lower_input or any(x in lower_input for x in glst()):\n",
    "#         session.criteria[\"gpu\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"gpu\"] = None\n",
    "#     if \"cpu\" in lower_input or any(x in lower_input for x in clst()):\n",
    "#         session.criteria[\"cpu\"] = user_input\n",
    "#     if \"ram\" in lower_input or any(x in lower_input for x in rlst()):\n",
    "#         session.criteria[\"ram\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"ram\"] = None\n",
    "#     if \"resolution\" in lower_input or any(x in lower_input for x in srlst()):\n",
    "#         session.criteria[\"resolution\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"resolution\"] = None\n",
    "#     if \"refresh\" in lower_input or any(x in lower_input for x in rrlst()):\n",
    "#         session.criteria[\"refresh rate\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"refresh rate\"] = None\n",
    "#     if \"display\" in lower_input or any(x in lower_input for x in dtlst()):\n",
    "#         session.criteria[\"display type\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"display type\"] = None\n",
    "#     if \"screen\" in lower_input or any(x in lower_input for x in sslst()):\n",
    "#         session.criteria[\"screen size\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"screen size\"] = None\n",
    "\n",
    "# def get_next_question(session: ChatbotSession) -> str:\n",
    "#     questions = {\n",
    "#         \"brand\": \"Of course! Let's start with the brand. Any preferences?\",\n",
    "#         \"gpu\": \"Noted! Which GPU are you aiming for?\",\n",
    "#         \"cpu\": \"Cool! Which processor do you prefer?\",\n",
    "#         \"ram\": \"Alright! What's your RAM requirement?\",\n",
    "#         \"resolution\": \"What screen resolution do you want?\",\n",
    "#         \"refresh rate\": \"What is the desired screen refresh rate?\",\n",
    "#         \"display type\": \"What type of screen do you prefer (IPS, OLED, ...)?\",\n",
    "#         \"screen size\": \"Great choice! What screen size are you looking for?\"\n",
    "#     }\n",
    "#     for key in questions:\n",
    "#         if key not in session.criteria:\n",
    "#             return questions[key]\n",
    "#     return \"🔍 Searching for the best options...\"\n",
    "\n",
    "# def process_user_input(user_id: str, user_input: str):\n",
    "#     session = get_session(user_id)\n",
    "    \n",
    "#     # Nếu câu nhập dài và chứa thông tin mô tả chi tiết, chuyển sang flow tìm kiếm\n",
    "#     if is_direct_search_query(user_input):\n",
    "#         session.current_flow = \"search\"\n",
    "#         return searching(user_input)\n",
    "    \n",
    "#     # Nếu câu hỏi dạng FAQ (ví dụ: \"what is gpu?\")\n",
    "#     if is_faq_question(user_input):\n",
    "#         prev_flow = session.current_flow\n",
    "#         faq_answer = get_faq_answer(user_input)\n",
    "#         session.current_flow = prev_flow\n",
    "#         return faq_answer\n",
    "    \n",
    "#     # Nếu đang trong guided flow, cập nhật tiêu chí và hỏi câu tiếp theo\n",
    "#     if session.current_flow == \"guided\":\n",
    "#         update_session_criteria(session, user_input)\n",
    "#         next_q = get_next_question(session)\n",
    "#         return next_q\n",
    "    \n",
    "#     return \"I'm sorry, I don't understand that question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: halo\n",
      "Chatbot: Of course! Let's start with the brand. Any preferences?\n",
      "Chatbot: Noted! Which GPU are you aiming for?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Alright! What's your RAM requirement?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What is the desired screen refresh rate?\n",
      "Chatbot: What is the desired screen refresh rate?\n",
      "Chatbot: What type of screen do you prefer (IPS, OLED, ...)?\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: 🔍 Searching for the best options...\n",
      "Chatbot: thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "# user_id = \"user_123\"\n",
    "# print(\"Chatbot: halo\")\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"User: \")\n",
    "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "#         print(\"Chatbot: thanks for chatting!\")\n",
    "#         break\n",
    "#     response = process_user_input(user_id, user_input)\n",
    "#     print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astorine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
