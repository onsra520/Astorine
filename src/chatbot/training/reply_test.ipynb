{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"__init__.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.append(str(Path(__file__).resolve().parents[1]))\n",
    "\n",
    "from utils.nlp_utils import tokenize, stem, bag_of_words\n",
    "from utils.ncomp import rlst, srlst, clst, glst, rrlst, dtlst, sslst, blst\n",
    "from handlers.rcm import searching\n",
    "# from nlp.ChatModel import NeuralNetwork, ChatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(__file__).resolve().parents[1]\n",
    "\n",
    "paths = {\n",
    "    \"intents\": os.path.abspath(f\"{project_root}/intents\"),\n",
    "    \"patterns\": os.path.abspath(f\"{project_root}/intents/patterns.json\"),\n",
    "    \"responses\": os.path.abspath(f\"{project_root}/intents/responses.json\"),\n",
    "    \"replymodel\": os.path.abspath(f\"{project_root}/models/replymodel.pth\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "intent_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.5783\n",
      "Epoch [200/500], Loss: 0.6499\n",
      "Epoch [300/500], Loss: 0.5844\n",
      "Epoch [400/500], Loss: 1.2764\n",
      "Epoch [500/500], Loss: 0.7132\n",
      "final loss: 0.7132\n",
      "training complete. file saved to C:\\Users\\trtie\\OneDrive - camann\\Documents\\GitHub - Repository\\Astorine\\src\\chatbot\\models\\replymodel.pth\n"
     ]
    }
   ],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.3):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "    def optimize(self, pruning_amount=0.3):\n",
    "        for _, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
    "        self.half()\n",
    "        scripted_model = torch.jit.script(self)\n",
    "        return scripted_model\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__init__(state['input_size'], state['hidden_size'], state['num_classes'], state['dropout_rate'])\n",
    "        self.load_state_dict(state['state_dict'])\n",
    "        \n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = Y_train\n",
    "\n",
    "    # H·ªó tr·ª£ vi·ªác truy c·∫≠p b·∫±ng ch·ªâ s·ªë ƒë·ªÉ c√≥ th·ªÉ l·∫•y m·∫´u th·ª© i trong t·∫≠p d·ªØ li·ªáu b·∫±ng c√°ch d√πng dataset[i].\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # Ch√∫ng ta c√≥ th·ªÉ g·ªçi len(dataset) ƒë·ªÉ tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc.\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "with open(paths[\"patterns\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    intents = json5.load(f)\n",
    "\n",
    "all_words, tags, xy = [], [], []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        word = tokenize(pattern)\n",
    "        all_words.extend(word)\n",
    "        xy.append((word, tag))\n",
    "        \n",
    "# X√≥a c√°c t·ª´ tr√πng l·∫∑p v√† s·∫Øp x·∫øp ch√∫ng\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: T√∫i t·ª´ cho m·ªói c√¢u m·∫´u (pattern_sentence).\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # Y: PyTorch CrossEntropyLoss ch·ªâ c·∫ßn class labels, kh√¥ng c·∫ßn one-hot.\n",
    "    label = tags.index(tag)\n",
    "    Y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "# Hyper-parameters - C√°c si√™u tham s·ªë\n",
    "num_epochs = 500 # S·ªë l·∫ßn l·∫∑p qua to√†n b·ªô d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "batch_size = 8 # S·ªë m·∫´u d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω c√πng m·ªôt l√∫c trong m·ªói l·∫ßn c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n",
    "learning_rate = 0.001 # T·ªëc ƒë·ªô h·ªçc, ƒëi·ªÅu ch·ªânh ƒë·ªô l·ªõn c·ªßa b∆∞·ªõc c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n",
    "input_size = len(X_train[0]) # K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o c·ªßa m√¥ h√¨nh, b·∫±ng s·ªë l∆∞·ª£ng t·ª´ trong b·ªô t·ª´ v·ª±ng (all_words).\n",
    "hidden_size = 8 # K√≠ch th∆∞·ªõc c·ªßa l·ªõp ·∫©n (hidden layer) trong m√¥ h√¨nh.\n",
    "output_size = len(tags) # K√≠ch th∆∞·ªõc ƒë·∫ßu ra c·ªßa m√¥ h√¨nh, b·∫±ng s·ªë l∆∞·ª£ng nh√£n (tags).\n",
    "\n",
    "dataset = DialogueDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # N·∫øu c√≥ GPU th√¨ s·ª≠ d·ª•ng, n·∫øu kh√¥ng th√¨ s·ª≠ d·ª•ng CPU\n",
    "model = CustomNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function v√† optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh qua nhi·ªÅu epoch.\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass - Lan truy·ªÅn ti·∫øn l√† qu√° tr√¨nh m√¥ h√¨nh t√≠nh to√°n ƒë·∫ßu ra (output) d·ª±a tr√™n d·ªØ li·ªáu ƒë·∫ßu v√†o (input)\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels) # T√≠nh to√°n loss gi·ªØa outputs v√† labels.\n",
    "\n",
    "        # Backward and optimize - Lan truy·ªÅn ng∆∞·ª£c v√† t·ªëi ∆∞u h√≥a\n",
    "        optimizer.zero_grad() # X√≥a gradient t·ª´ tr∆∞·ªõc ƒë√≥, tr√°nh vi·ªác gradient t√≠ch l≈©y.\n",
    "        loss.backward() # th·ª±c hi·ªán lan truy·ªÅn ng∆∞·ª£c, t√≠nh to√°n gradient c·ªßa loss ƒë·ªëi v·ªõi t·∫•t c·∫£ c√°c tham s·ªë c·ªßa m√¥ h√¨nh b·∫±ng c√°ch s·ª≠ d·ª•ng quy t·∫Øc chu·ªói (chain rule).\n",
    "        optimizer.step() # c·∫≠p nh·∫≠t tr·ªçng s·ªë c·ªßa m√¥ h√¨nh d·ª±a tr√™n gradient ƒë√£ t√≠nh to√°n.\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "torch.save(data, paths[\"replymodel\"]) \n",
    "print(f'training complete. file saved to {paths[\"replymodel\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: halo\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(paths[\"replymodel\"])\n",
    "model = CustomNN(data[\"input_size\"], data[\"hidden_size\"], data[\"output_size\"]).to(device)\n",
    "model.load_state_dict(data[\"model_state\"])\n",
    "model.eval()\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "def predict_intent(user_input):\n",
    "    words = tokenize(user_input)\n",
    "    bag = bag_of_words(words, all_words)\n",
    "    bag = torch.from_numpy(bag).float().to(device)\n",
    "    bag = bag.unsqueeze(0)  # Th√™m batch dimension: [input_size] -> [1, input_size]\n",
    "    with torch.no_grad():\n",
    "        output = model(bag)  # output s·∫Ω c√≥ k√≠ch th∆∞·ªõc [1, num_classes]\n",
    "        _, predicted = torch.max(output, dim=1)  # predicted s·∫Ω l√† [1]\n",
    "        tag = tags[predicted.item()]\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        if prob.item() > 0.75:  # Ng∆∞·ª°ng tin c·∫≠y\n",
    "            return tag\n",
    "        return \"unknown\"\n",
    "\n",
    "sessions = {}\n",
    "\n",
    "class ChatbotSession:\n",
    "    def __init__(self):\n",
    "        self.current_flow = \"guided\"  # 'guided' ho·∫∑c 'search'\n",
    "        self.context = None           # v√≠ d·ª•: \"Need_help\", \"collecting_criteria\", v.v.\n",
    "        self.criteria = {}            # l∆∞u tr·ªØ c√°c ti√™u ch√≠: brand, gpu, cpu, ...\n",
    "        self.previous_flow = None     # l∆∞u flow t·∫°m th·ªùi khi tr·∫£ l·ªùi FAQ\n",
    "\n",
    "def get_session(user_id):\n",
    "    if user_id not in sessions:\n",
    "        sessions[user_id] = ChatbotSession()\n",
    "    return sessions[user_id]\n",
    "\n",
    "def is_faq_question(text: str) -> bool:\n",
    "    faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "    return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "\n",
    "def is_direct_search_query(text: str) -> bool:\n",
    "    # N·∫øu c√¢u c√≥ nhi·ªÅu t·ª´ v√† ch·ª©a c√°c t·ª´ kh√≥a ch·ªâ ra y√™u c·∫ßu t√¨m ki·∫øm laptop\n",
    "    return len(text.split()) > 15 and any(term in text.lower() for term in [\"suggest\", \"recommend\", \"find\", \"laptop\"])\n",
    "\n",
    "def is_faq_question(text: str) -> bool:\n",
    "    faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "    return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "\n",
    "def get_faq_answer(query: str) -> str:\n",
    "    faq_dict = {\n",
    "        \"what is gpu\": \"GPU (Graphics Processing Unit).\",\n",
    "        \"what is cpu\": \"CPU (Central Processing Unit).\",\n",
    "        \"what is ram\": \"RAM (Random Access Memory).\"\n",
    "    }\n",
    "    for key in faq_dict:\n",
    "        if key in query.lower():\n",
    "            return faq_dict[key]\n",
    "    return \"I don't have the answer to that question.\"\n",
    "\n",
    "def update_session_criteria(session: ChatbotSession, user_input: str):\n",
    "    lower_input = user_input.lower()\n",
    "    if \"brand\" in lower_input or any(brand in lower_input for brand in blst()):\n",
    "        session.criteria[\"brand\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"brand\"] = None\n",
    "    if \"gpu\" in lower_input or any(x in lower_input for x in glst()):\n",
    "        session.criteria[\"gpu\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"gpu\"] = None\n",
    "    if \"cpu\" in lower_input or any(x in lower_input for x in clst()):\n",
    "        session.criteria[\"cpu\"] = user_input\n",
    "    if \"ram\" in lower_input or any(x in lower_input for x in rlst()):\n",
    "        session.criteria[\"ram\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"ram\"] = None\n",
    "    if \"resolution\" in lower_input or any(x in lower_input for x in srlst()):\n",
    "        session.criteria[\"resolution\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"resolution\"] = None\n",
    "    if \"refresh\" in lower_input or any(x in lower_input for x in rrlst()):\n",
    "        session.criteria[\"refresh rate\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"refresh rate\"] = None\n",
    "    if \"display\" in lower_input or any(x in lower_input for x in dtlst()):\n",
    "        session.criteria[\"display type\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"display type\"] = None\n",
    "    if \"screen\" in lower_input or any(x in lower_input for x in sslst()):\n",
    "        session.criteria[\"screen size\"] = user_input\n",
    "    else:\n",
    "        session.criteria[\"screen size\"] = None\n",
    "\n",
    "def get_next_question(session: ChatbotSession) -> str:\n",
    "    questions = {\n",
    "        \"brand\": \"Of course! Let's start with the brand. Any preferences?\",\n",
    "        \"gpu\": \"Noted! Which GPU are you aiming for?\",\n",
    "        \"cpu\": \"Cool! Which processor do you prefer?\",\n",
    "        \"ram\": \"Alright! What's your RAM requirement?\",\n",
    "        \"resolution\": \"What screen resolution do you want?\",\n",
    "        \"refresh rate\": \"What is the desired screen refresh rate?\",\n",
    "        \"display type\": \"What type of screen do you prefer (IPS, OLED, ...)?\",\n",
    "        \"screen size\": \"Great choice! What screen size are you looking for?\"\n",
    "    }\n",
    "    for key in questions:\n",
    "        if key not in session.criteria:\n",
    "            return questions[key]\n",
    "    return \"üîç Searching for the best options...\"\n",
    "\n",
    "def process_user_input(user_id: str, user_input: str):\n",
    "    session = get_session(user_id)\n",
    "    intent = predict_intent(user_input)\n",
    "\n",
    "    # X·ª≠ l√Ω c√°c √Ω ƒë·ªãnh c·ª• th·ªÉ\n",
    "    if intent == \"search\" or is_direct_search_query(user_input):\n",
    "        session.current_flow = \"search\"\n",
    "        return searching(user_input)\n",
    "    \n",
    "    elif intent == \"faq\" or is_faq_question(user_input):\n",
    "        prev_flow = session.current_flow\n",
    "        faq_answer = get_faq_answer(user_input)\n",
    "        session.current_flow = prev_flow\n",
    "        return faq_answer\n",
    "    \n",
    "    elif intent == \"start_guided\":\n",
    "        session.current_flow = \"guided\"\n",
    "        session.context = \"collecting_brand\"\n",
    "        return \"Let's start with the brand. Any preferences?\"\n",
    "    \n",
    "    elif session.current_flow == \"guided\":\n",
    "        update_session_criteria(session, user_input)\n",
    "        next_q = get_next_question(session)\n",
    "        if next_q == \"üîç Searching for the best options...\":\n",
    "            return searching(session.criteria)\n",
    "        return next_q\n",
    "    \n",
    "    return \"I'm sorry, I don't understand that question.\"\n",
    "\n",
    "# Sau khi hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh\n",
    "data = torch.load(paths[\"replymodel\"])\n",
    "model = CustomNN(data[\"input_size\"], data[\"hidden_size\"], data[\"output_size\"]).to(device)\n",
    "model.load_state_dict(data[\"model_state\"])\n",
    "model.eval()\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "user_id = \"user_123\"\n",
    "print(\"Chatbot: halo\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: thanks for chatting!\")\n",
    "        break\n",
    "    response = process_user_input(user_id, user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_direct_search_query(text: str) -> bool:\n",
    "#     # N·∫øu c√¢u c√≥ nhi·ªÅu t·ª´ v√† ch·ª©a c√°c t·ª´ kh√≥a ch·ªâ ra y√™u c·∫ßu t√¨m ki·∫øm laptop\n",
    "#     return len(text.split()) > 15 and any(term in text.lower() for term in [\"suggest\", \"recommend\", \"find\", \"laptop\"])\n",
    "\n",
    "# def is_faq_question(text: str) -> bool:\n",
    "#     faq_triggers = [\"what is\", \"explain\", \"how does\"]\n",
    "#     return any(text.lower().startswith(trigger) for trigger in faq_triggers)\n",
    "# def get_faq_answer(query: str) -> str:\n",
    "#     faq_dict = {\n",
    "#         \"what is gpu\": \"GPU (Graphics Processing Unit).\",\n",
    "#         \"what is cpu\": \"CPU (Central Processing Unit).\",\n",
    "#         \"what is ram\": \"RAM (Random Access Memory).\"\n",
    "#     }\n",
    "#     for key in faq_dict:\n",
    "#         if key in query.lower():\n",
    "#             return faq_dict[key]\n",
    "#     return \"I don't have the answer to that question.\"\n",
    "\n",
    "# def update_session_criteria(session: ChatbotSession, user_input: str):\n",
    "#     lower_input = user_input.lower()\n",
    "#     if \"brand\" in lower_input or any(brand in lower_input for brand in blst()):\n",
    "#         session.criteria[\"brand\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"brand\"] = None\n",
    "#     if \"gpu\" in lower_input or any(x in lower_input for x in glst()):\n",
    "#         session.criteria[\"gpu\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"gpu\"] = None\n",
    "#     if \"cpu\" in lower_input or any(x in lower_input for x in clst()):\n",
    "#         session.criteria[\"cpu\"] = user_input\n",
    "#     if \"ram\" in lower_input or any(x in lower_input for x in rlst()):\n",
    "#         session.criteria[\"ram\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"ram\"] = None\n",
    "#     if \"resolution\" in lower_input or any(x in lower_input for x in srlst()):\n",
    "#         session.criteria[\"resolution\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"resolution\"] = None\n",
    "#     if \"refresh\" in lower_input or any(x in lower_input for x in rrlst()):\n",
    "#         session.criteria[\"refresh rate\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"refresh rate\"] = None\n",
    "#     if \"display\" in lower_input or any(x in lower_input for x in dtlst()):\n",
    "#         session.criteria[\"display type\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"display type\"] = None\n",
    "#     if \"screen\" in lower_input or any(x in lower_input for x in sslst()):\n",
    "#         session.criteria[\"screen size\"] = user_input\n",
    "#     else:\n",
    "#         session.criteria[\"screen size\"] = None\n",
    "\n",
    "# def get_next_question(session: ChatbotSession) -> str:\n",
    "#     questions = {\n",
    "#         \"brand\": \"Of course! Let's start with the brand. Any preferences?\",\n",
    "#         \"gpu\": \"Noted! Which GPU are you aiming for?\",\n",
    "#         \"cpu\": \"Cool! Which processor do you prefer?\",\n",
    "#         \"ram\": \"Alright! What's your RAM requirement?\",\n",
    "#         \"resolution\": \"What screen resolution do you want?\",\n",
    "#         \"refresh rate\": \"What is the desired screen refresh rate?\",\n",
    "#         \"display type\": \"What type of screen do you prefer (IPS, OLED, ...)?\",\n",
    "#         \"screen size\": \"Great choice! What screen size are you looking for?\"\n",
    "#     }\n",
    "#     for key in questions:\n",
    "#         if key not in session.criteria:\n",
    "#             return questions[key]\n",
    "#     return \"üîç Searching for the best options...\"\n",
    "\n",
    "# def process_user_input(user_id: str, user_input: str):\n",
    "#     session = get_session(user_id)\n",
    "    \n",
    "#     # N·∫øu c√¢u nh·∫≠p d√†i v√† ch·ª©a th√¥ng tin m√¥ t·∫£ chi ti·∫øt, chuy·ªÉn sang flow t√¨m ki·∫øm\n",
    "#     if is_direct_search_query(user_input):\n",
    "#         session.current_flow = \"search\"\n",
    "#         return searching(user_input)\n",
    "    \n",
    "#     # N·∫øu c√¢u h·ªèi d·∫°ng FAQ (v√≠ d·ª•: \"what is gpu?\")\n",
    "#     if is_faq_question(user_input):\n",
    "#         prev_flow = session.current_flow\n",
    "#         faq_answer = get_faq_answer(user_input)\n",
    "#         session.current_flow = prev_flow\n",
    "#         return faq_answer\n",
    "    \n",
    "#     # N·∫øu ƒëang trong guided flow, c·∫≠p nh·∫≠t ti√™u ch√≠ v√† h·ªèi c√¢u ti·∫øp theo\n",
    "#     if session.current_flow == \"guided\":\n",
    "#         update_session_criteria(session, user_input)\n",
    "#         next_q = get_next_question(session)\n",
    "#         return next_q\n",
    "    \n",
    "#     return \"I'm sorry, I don't understand that question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: halo\n",
      "Chatbot: Of course! Let's start with the brand. Any preferences?\n",
      "Chatbot: Noted! Which GPU are you aiming for?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Cool! Which processor do you prefer?\n",
      "Chatbot: Alright! What's your RAM requirement?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What screen resolution do you want?\n",
      "Chatbot: What is the desired screen refresh rate?\n",
      "Chatbot: What is the desired screen refresh rate?\n",
      "Chatbot: What type of screen do you prefer (IPS, OLED, ...)?\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: üîç Searching for the best options...\n",
      "Chatbot: thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "# user_id = \"user_123\"\n",
    "# print(\"Chatbot: halo\")\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"User: \")\n",
    "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "#         print(\"Chatbot: thanks for chatting!\")\n",
    "#         break\n",
    "#     response = process_user_input(user_id, user_input)\n",
    "#     print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astorine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
